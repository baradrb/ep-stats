{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Ep-Stats \u00b6 Statistical package for experimentation platform. It provides a general python package and REST API that can be used to evaluate any metric in AB test experiment. Features \u00b6 Robust two-tailed t-test implementation with multiple p-value corrections and delta-methods applied. Sequential evaluations allowing to stop experiments early. Connect it to any data source to either get pre-aggregated or per randomization unit data. Simple expression language to define arbitrary metrics. REST API to integrate it as a service in experimentation portal with score cards. We encourage all readers to get familiar with basic EP Principles and then follow Quick Start . Architecture \u00b6 In regular experimentation platform, client data and telemetry have to pass through several components before experimenters can see results in metrics in scorecards. Ep-stats solves the statistical part of the pipeline as is described on following image. Client data and telemetry collection are specific to the company, we do not strive to provide any support for this part. Aggregator is optional part between raw data and ep-stats that can help to unify and pre-aggregate data consumed by ep-stats. Scorecards represent user interface in some kind of experimentation portal or knowledge base that lists experiments and displays scorecards with experiment results and statistics. Ep-stats offers following components: DAO (data access object) interfacing underlying data source with a way how to compile required metric data into SQL or anything else in use. Stats computing experiment evaluation with statistics. REST API a web app that makes it easy to integrate experiment evaluations in scorecards. Known Limitations and Suggestion for Future Work \u00b6 Field of online experimentation is developing as well as data, metrics, methods, statistics. We strive to provide correct experiment evaluation. Its development takes time. Current main and known limitations. Metrics that are not based on (randomization) unit type (e.g. Views per User if session is our unit type) require application of delta method and bootstrapping 1 . This is not implemented yet. Drill-down into dimensions is not implemented yet. Experiment data are aggregated as whole, cumulative evaluation or data for timeline graphs are not yet implemented. Origin \u00b6 Ep-stats originated as a part of experimentation platform implementation in Avast . While there are many books on experimentation and statistics, there are few or none good implementations of it. We aim to fill in this gap between theory and practice by open-sourcing ep-stats package. We have been using EP with this implementation of ep-stats to run and evaluate hundreds of experiments in Avast. We will be adding new stuff here as we improve it and test it in Avast. Inspiration \u00b6 Software engineering practices of this package have been heavily inspired by marvelous calmcode.io site managed by Vincent D. Warmerdam . A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas \u21a9","title":"Introduction"},{"location":"index.html#ep-stats","text":"Statistical package for experimentation platform. It provides a general python package and REST API that can be used to evaluate any metric in AB test experiment.","title":"Ep-Stats"},{"location":"index.html#features","text":"Robust two-tailed t-test implementation with multiple p-value corrections and delta-methods applied. Sequential evaluations allowing to stop experiments early. Connect it to any data source to either get pre-aggregated or per randomization unit data. Simple expression language to define arbitrary metrics. REST API to integrate it as a service in experimentation portal with score cards. We encourage all readers to get familiar with basic EP Principles and then follow Quick Start .","title":"Features"},{"location":"index.html#architecture","text":"In regular experimentation platform, client data and telemetry have to pass through several components before experimenters can see results in metrics in scorecards. Ep-stats solves the statistical part of the pipeline as is described on following image. Client data and telemetry collection are specific to the company, we do not strive to provide any support for this part. Aggregator is optional part between raw data and ep-stats that can help to unify and pre-aggregate data consumed by ep-stats. Scorecards represent user interface in some kind of experimentation portal or knowledge base that lists experiments and displays scorecards with experiment results and statistics. Ep-stats offers following components: DAO (data access object) interfacing underlying data source with a way how to compile required metric data into SQL or anything else in use. Stats computing experiment evaluation with statistics. REST API a web app that makes it easy to integrate experiment evaluations in scorecards.","title":"Architecture"},{"location":"index.html#known-limitations-and-suggestion-for-future-work","text":"Field of online experimentation is developing as well as data, metrics, methods, statistics. We strive to provide correct experiment evaluation. Its development takes time. Current main and known limitations. Metrics that are not based on (randomization) unit type (e.g. Views per User if session is our unit type) require application of delta method and bootstrapping 1 . This is not implemented yet. Drill-down into dimensions is not implemented yet. Experiment data are aggregated as whole, cumulative evaluation or data for timeline graphs are not yet implemented.","title":"Known Limitations and Suggestion for Future Work"},{"location":"index.html#origin","text":"Ep-stats originated as a part of experimentation platform implementation in Avast . While there are many books on experimentation and statistics, there are few or none good implementations of it. We aim to fill in this gap between theory and practice by open-sourcing ep-stats package. We have been using EP with this implementation of ep-stats to run and evaluate hundreds of experiments in Avast. We will be adding new stuff here as we improve it and test it in Avast.","title":"Origin"},{"location":"index.html#inspiration","text":"Software engineering practices of this package have been heavily inspired by marvelous calmcode.io site managed by Vincent D. Warmerdam . A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas \u21a9","title":"Inspiration"},{"location":"principles.html","text":"Basic Principles \u00b6 We describe basic principles of online controlled experiments and a way how we implement them as events, goals, metrics, and exposures. This section also gives some implementation advice how to effectively set telemetry in the app to collect events. Online Controlled Experiments \u00b6 When executed properly, online controlled experiments (OCE) are a way to establish causal relationship between some introduced change (in treatment variant) and its impact on some metric of interest. OCEs randomly distribute Randomization Units (RU) (e.g. users) into multiple groups called variants. Randomization units are also known as experimental units or observations in experimentation literature. We will be using simple term unit to refer to randomization unit in this documentation. We compare the Control variant where there is no treatment or change present with one or more Treatment variant(s) where there is treatment or change present. We are interested in which variant is performing how compared to the control variant. Units' interactions are recorded during the period of the experiment and later analyzed statistically during the experiment and at the end of it. We are interested in: Determining the presence of a difference in one or more metrics between the control and treatment variants Determining the size of the difference Determining if we can reason that measured difference was caused by the tested change or by pure chance If we are confident (up to some level) that the measured difference was caused by the tested change we consider the diff to be Statistically Significant (with some confidence level). Colloquially put, to evaluate some metric in the experiment, we need a way how to calculate the metric from goals that we collect in form of events from units exposed to the experiment. We will continue by describing these terms and explaining how they play together. Exposures \u00b6 We distribute units randomly (independently and identically \u2013 IID 1 ) into experiment variants. We can imagine that we assign units to some experiment and variant. We say that the unit (e.g. user, license, ...) has been exposed to the experiment E and variant V at the moment when the unit experienced the control or treatment variants. For example, we expose the unit to the green variant of some experiment when the unit sees the green screen. Or we expose some other unit to the yellow variant of the same experiment when the unit sees the yellow screen. The concept of explicit exposure is crucial in ep-stats. It decouples assignment of the unit into some experiment and variant from rather explicit exposure. The assignment has no impact on the unit's customer experience, it is merely just a technical thing. While the moment of exposure has direct impact on the unit's customer experience because the unit sees a green or yellow screen. The following graph illustrates the importance of the exposure event. It is the moment we start recording events attributed with the exposed unit ( unit-1 and later unit-2 in this case) per experiment and variant. By making the exposure explicit, we later limit our analysis only to those units that experienced the change in the experiment. This drastically improves sensitivity of the analysis (we are able to detect smaller differences or the same diffs but with lower unit count). Since we know the explicit moment of exposure, we do not need to add any artificial test information to events we collect from products. We simply start recording all events from all units that have already been exposed to experiment E and its variant V. One unit can certainly be exposed into an arbitrary number of experiments at the same time. We record all events from such unit and attribute them to all experiments and variants the unit has been exposed to. Events and Goals \u00b6 After the unit has been exposed to the experiment (control or treatment variant), we are interested in unit's events such as clicks, conversions etc. These events are called events in ep-stats. ep-stats listens to the stream of these events coming from units and we attribute (record) events of interest as goals to some experiment E and variant V if that event happened on some unit that has been exposed to the experiment E and variant V before. ep-stats uses goals to compute all metrics. The exposure is a (special) goal in the ep-stats as well. Goal Attribution \u00b6 We attribute goals to experiments and variants based on units exposures. This means that every event that should make a goal in ep-stats must reference some unit. Without that, ep-stats is not able to recognize what exposure to look up to determine which experiment(s) and variant(s) the goal should be attributed to. See Supported Unit Types . Metrics \u00b6 We calculate metrics from goals. We calculate metrics per experiment variant. Metrics are usually some average number (or value, e.g., value of conversion in USD) of positive goals over number of exposures. For example, the metric Average Bookings is defined as USD value of all conversions of all users exposed to the experiment divided by number of users exposed to the experiment. We define metrics from goals in form of nominator and denominator . For example, we define metric Average Bookings as value(test_unit_type.unit.conversion) / count(test_unit_type.global.exposure) where value determines we need the value of goals recorded, e.g., USD bookings in this case ( count then means number of conversion goals) test_unit_type is a type of unit unit , global are types of aggregation conversion , exposure are goals See Aggregation for details about how we process data and about specifics of metric definitions in ep-stats. Some Potential Caveats \u00b6 While ep-stats handles most of the known problems transparently for the experimenter, it is important to be mindful at least about the following potential problems. Due to randomization of units, every metric is a random variable. ep-stats helps to provide \"always valid\" metric values and statistics to allow you to \"always make a good decision\". This relates to a problem of \"peeking at results\" which we solve using sequential analysis . In some metrics, a unit is not the same as analysis unit. This violates IID assumption and requires ep-stats to perform delta method . We describe these problems in detail in Statistics . Independent and identically distributed random variables - IID \u21a9","title":"Principles"},{"location":"principles.html#basic-principles","text":"We describe basic principles of online controlled experiments and a way how we implement them as events, goals, metrics, and exposures. This section also gives some implementation advice how to effectively set telemetry in the app to collect events.","title":"Basic Principles"},{"location":"principles.html#online-controlled-experiments","text":"When executed properly, online controlled experiments (OCE) are a way to establish causal relationship between some introduced change (in treatment variant) and its impact on some metric of interest. OCEs randomly distribute Randomization Units (RU) (e.g. users) into multiple groups called variants. Randomization units are also known as experimental units or observations in experimentation literature. We will be using simple term unit to refer to randomization unit in this documentation. We compare the Control variant where there is no treatment or change present with one or more Treatment variant(s) where there is treatment or change present. We are interested in which variant is performing how compared to the control variant. Units' interactions are recorded during the period of the experiment and later analyzed statistically during the experiment and at the end of it. We are interested in: Determining the presence of a difference in one or more metrics between the control and treatment variants Determining the size of the difference Determining if we can reason that measured difference was caused by the tested change or by pure chance If we are confident (up to some level) that the measured difference was caused by the tested change we consider the diff to be Statistically Significant (with some confidence level). Colloquially put, to evaluate some metric in the experiment, we need a way how to calculate the metric from goals that we collect in form of events from units exposed to the experiment. We will continue by describing these terms and explaining how they play together.","title":"Online Controlled Experiments"},{"location":"principles.html#exposures","text":"We distribute units randomly (independently and identically \u2013 IID 1 ) into experiment variants. We can imagine that we assign units to some experiment and variant. We say that the unit (e.g. user, license, ...) has been exposed to the experiment E and variant V at the moment when the unit experienced the control or treatment variants. For example, we expose the unit to the green variant of some experiment when the unit sees the green screen. Or we expose some other unit to the yellow variant of the same experiment when the unit sees the yellow screen. The concept of explicit exposure is crucial in ep-stats. It decouples assignment of the unit into some experiment and variant from rather explicit exposure. The assignment has no impact on the unit's customer experience, it is merely just a technical thing. While the moment of exposure has direct impact on the unit's customer experience because the unit sees a green or yellow screen. The following graph illustrates the importance of the exposure event. It is the moment we start recording events attributed with the exposed unit ( unit-1 and later unit-2 in this case) per experiment and variant. By making the exposure explicit, we later limit our analysis only to those units that experienced the change in the experiment. This drastically improves sensitivity of the analysis (we are able to detect smaller differences or the same diffs but with lower unit count). Since we know the explicit moment of exposure, we do not need to add any artificial test information to events we collect from products. We simply start recording all events from all units that have already been exposed to experiment E and its variant V. One unit can certainly be exposed into an arbitrary number of experiments at the same time. We record all events from such unit and attribute them to all experiments and variants the unit has been exposed to.","title":"Exposures"},{"location":"principles.html#events-and-goals","text":"After the unit has been exposed to the experiment (control or treatment variant), we are interested in unit's events such as clicks, conversions etc. These events are called events in ep-stats. ep-stats listens to the stream of these events coming from units and we attribute (record) events of interest as goals to some experiment E and variant V if that event happened on some unit that has been exposed to the experiment E and variant V before. ep-stats uses goals to compute all metrics. The exposure is a (special) goal in the ep-stats as well.","title":"Events and Goals"},{"location":"principles.html#goal-attribution","text":"We attribute goals to experiments and variants based on units exposures. This means that every event that should make a goal in ep-stats must reference some unit. Without that, ep-stats is not able to recognize what exposure to look up to determine which experiment(s) and variant(s) the goal should be attributed to. See Supported Unit Types .","title":"Goal Attribution"},{"location":"principles.html#metrics","text":"We calculate metrics from goals. We calculate metrics per experiment variant. Metrics are usually some average number (or value, e.g., value of conversion in USD) of positive goals over number of exposures. For example, the metric Average Bookings is defined as USD value of all conversions of all users exposed to the experiment divided by number of users exposed to the experiment. We define metrics from goals in form of nominator and denominator . For example, we define metric Average Bookings as value(test_unit_type.unit.conversion) / count(test_unit_type.global.exposure) where value determines we need the value of goals recorded, e.g., USD bookings in this case ( count then means number of conversion goals) test_unit_type is a type of unit unit , global are types of aggregation conversion , exposure are goals See Aggregation for details about how we process data and about specifics of metric definitions in ep-stats.","title":"Metrics"},{"location":"principles.html#some-potential-caveats","text":"While ep-stats handles most of the known problems transparently for the experimenter, it is important to be mindful at least about the following potential problems. Due to randomization of units, every metric is a random variable. ep-stats helps to provide \"always valid\" metric values and statistics to allow you to \"always make a good decision\". This relates to a problem of \"peeking at results\" which we solve using sequential analysis . In some metrics, a unit is not the same as analysis unit. This violates IID assumption and requires ep-stats to perform delta method . We describe these problems in detail in Statistics . Independent and identically distributed random variables - IID \u21a9","title":"Some Potential Caveats"},{"location":"resources.html","text":"Resources \u00b6 Following are selected lists of publications that influenced us the most in making ep-stats design decisions and development. The most important being Ron Kohavi, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing that we used for inspiration on soft topics like how to build experimentation culture and protocol and as a checklist to make sure we haven't forgot anything in implementation. Ron Kohavi, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing Georgi Z. Georgiev, Statistical Methods in Online A/B Testing: Statistics for data-driven business decisions and risk management in e-commerce ExP Experimentation Platform A. Fabijan et al., Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale R. Kohavi, Sample Ratio Mismatch A. Deng et al., Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data Yandex, Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics R. Kohavi et al., Seven Rules of Thumb for Web Site Experiments H. Hohnhold et al., Focusing on the Long-term: It's Good for Users and Business Yandex, Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics Various authors, Top Challenges from the first Practical Online Controlled Experiments Summit David L. DeMents, K. K. Gordon Lan, Interim analysis: The alpha spending function approach P. C. O\u2019Brien, T.R. Fleming - A Multiple Testing Procedure for Clinical Trials A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas","title":"Resources"},{"location":"resources.html#resources","text":"Following are selected lists of publications that influenced us the most in making ep-stats design decisions and development. The most important being Ron Kohavi, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing that we used for inspiration on soft topics like how to build experimentation culture and protocol and as a checklist to make sure we haven't forgot anything in implementation. Ron Kohavi, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing Georgi Z. Georgiev, Statistical Methods in Online A/B Testing: Statistics for data-driven business decisions and risk management in e-commerce ExP Experimentation Platform A. Fabijan et al., Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale R. Kohavi, Sample Ratio Mismatch A. Deng et al., Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data Yandex, Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics R. Kohavi et al., Seven Rules of Thumb for Web Site Experiments H. Hohnhold et al., Focusing on the Long-term: It's Good for Users and Business Yandex, Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics Various authors, Top Challenges from the first Practical Online Controlled Experiments Summit David L. DeMents, K. K. Gordon Lan, Interim analysis: The alpha spending function approach P. C. O\u2019Brien, T.R. Fleming - A Multiple Testing Procedure for Clinical Trials A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas","title":"Resources"},{"location":"api/check.html","text":"Check \u00b6 \u00b6 Perform data quality check that accompanies metric evaluation in the experiment. See Data Quality Checks for details about data quality checks and [ Evaluation ][epstats.toolkit.experiment.Evaluation] for description of output. evaluate_agg ( self , goals , default_exp_variant_id ) \u00b6 Evaluate this check from pre-aggregated goals. Parameters: Name Type Description Default goals DataFrame one row per experiment variant required default_exp_variant_id str default variant required See [ Experiment.evaluate_agg ][epstats.toolkit.experiment.Experiment.evaluate_agg] for details on goals at input. Returns: Type Description DataFrame checks dataframe with columns: checks dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id check_id - check id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition variable_id - name of the variable in check evaluation, SRM check has following variables p_value , test_stat , confidence_level value - value of the variable Source code in epstats/toolkit/check.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def evaluate_agg ( self , goals : pd . DataFrame , default_exp_variant_id : str ) -> pd . DataFrame : \"\"\" Evaluate this check from pre-aggregated goals. Arguments: goals: one row per experiment variant default_exp_variant_id: default variant See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for details on `goals` at input. Returns: `checks` dataframe with columns: `checks` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`, `test_stat`, `confidence_level` 1. `value` - value of the variable \"\"\" raise NotImplementedError () evaluate_by_unit ( self , goals , default_exp_variant_id ) \u00b6 Evaluate this check from goals aggregated by unit. Parameters: Name Type Description Default goals DataFrame ne row per experiment variant required default_exp_variant_id str default variant required See [ Experiment.evaluate_by_unit ][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for details on goals at input. Returns: Type Description DataFrame checks dataframe with columns: checks dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id check_id - check id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition variable_id - name of the variable in check evaluation, SRM check has following variables p_value , test_stat , confidence_level value - value of the variable Source code in epstats/toolkit/check.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def evaluate_by_unit ( self , goals : pd . DataFrame , default_exp_variant_id : str ) -> pd . DataFrame : \"\"\" Evaluate this check from goals aggregated by unit. Arguments: goals: ne row per experiment variant default_exp_variant_id: default variant See [`Experiment.evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for details on `goals` at input. Returns: `checks` dataframe with columns: `checks` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`, `test_stat`, `confidence_level` 1. `value` - value of the variable \"\"\" raise NotImplementedError () get_goals ( self ) \u00b6 List of all goals needed to evaluate the check in the experiment. Returns: Type Description List list of parsed structured goals Source code in epstats/toolkit/check.py 30 31 32 33 34 35 36 37 def get_goals ( self ) -> List : \"\"\" List of all goals needed to evaluate the check in the experiment. Returns: list of parsed structured goals \"\"\" return self . _goals","title":"Check"},{"location":"api/check.html#check","text":"","title":"Check"},{"location":"api/check.html#epstats.toolkit.check.Check","text":"Perform data quality check that accompanies metric evaluation in the experiment. See Data Quality Checks for details about data quality checks and [ Evaluation ][epstats.toolkit.experiment.Evaluation] for description of output.","title":"epstats.toolkit.check.Check"},{"location":"api/check.html#epstats.toolkit.check.Check.evaluate_agg","text":"Evaluate this check from pre-aggregated goals. Parameters: Name Type Description Default goals DataFrame one row per experiment variant required default_exp_variant_id str default variant required See [ Experiment.evaluate_agg ][epstats.toolkit.experiment.Experiment.evaluate_agg] for details on goals at input. Returns: Type Description DataFrame checks dataframe with columns: checks dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id check_id - check id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition variable_id - name of the variable in check evaluation, SRM check has following variables p_value , test_stat , confidence_level value - value of the variable Source code in epstats/toolkit/check.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def evaluate_agg ( self , goals : pd . DataFrame , default_exp_variant_id : str ) -> pd . DataFrame : \"\"\" Evaluate this check from pre-aggregated goals. Arguments: goals: one row per experiment variant default_exp_variant_id: default variant See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for details on `goals` at input. Returns: `checks` dataframe with columns: `checks` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`, `test_stat`, `confidence_level` 1. `value` - value of the variable \"\"\" raise NotImplementedError ()","title":"evaluate_agg()"},{"location":"api/check.html#epstats.toolkit.check.Check.evaluate_by_unit","text":"Evaluate this check from goals aggregated by unit. Parameters: Name Type Description Default goals DataFrame ne row per experiment variant required default_exp_variant_id str default variant required See [ Experiment.evaluate_by_unit ][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for details on goals at input. Returns: Type Description DataFrame checks dataframe with columns: checks dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id check_id - check id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition variable_id - name of the variable in check evaluation, SRM check has following variables p_value , test_stat , confidence_level value - value of the variable Source code in epstats/toolkit/check.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def evaluate_by_unit ( self , goals : pd . DataFrame , default_exp_variant_id : str ) -> pd . DataFrame : \"\"\" Evaluate this check from goals aggregated by unit. Arguments: goals: ne row per experiment variant default_exp_variant_id: default variant See [`Experiment.evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for details on `goals` at input. Returns: `checks` dataframe with columns: `checks` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`, `test_stat`, `confidence_level` 1. `value` - value of the variable \"\"\" raise NotImplementedError ()","title":"evaluate_by_unit()"},{"location":"api/check.html#epstats.toolkit.check.Check.get_goals","text":"List of all goals needed to evaluate the check in the experiment. Returns: Type Description List list of parsed structured goals Source code in epstats/toolkit/check.py 30 31 32 33 34 35 36 37 def get_goals ( self ) -> List : \"\"\" List of all goals needed to evaluate the check in the experiment. Returns: list of parsed structured goals \"\"\" return self . _goals","title":"get_goals()"},{"location":"api/dao.html","text":"Dao \u00b6 \u00b6 Abstract class interfacing any kind of underlying data source. close ( self ) \u00b6 Close underlying data source connection and frees resources (if any). Source code in epstats/toolkit/dao.py 31 32 33 34 35 def close ( self ) -> None : \"\"\" Close underlying data source connection and frees resources (if any). \"\"\" pass get_agg_goals ( self , experiment ) \u00b6 Get goals data pre-aggregated by columns exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value . See [ Experiment.evaluate_agg ][epstats.toolkit.experiment.Experiment.evaluate_agg] for column descriptions and example result. Source code in epstats/toolkit/dao.py 21 22 23 24 25 26 27 28 29 def get_agg_goals ( self , experiment : Experiment ) -> pd . DataFrame : \"\"\" Get goals data pre-aggregated by columns `exp_variant_id`, `unit_type`, `agg_type`, `goal`, `dimension`, `dimension_value`. See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for column descriptions and example result. \"\"\" pass get_unit_goals ( self , experiment ) \u00b6 Get goals data pre-aggregated by columns exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value and unit_id . See [ Experiment.evaluate_by_unit ][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for column descriptions and example result. Source code in epstats/toolkit/dao.py 11 12 13 14 15 16 17 18 19 def get_unit_goals ( self , experiment : Experiment ) -> pd . DataFrame : \"\"\" Get goals data pre-aggregated by columns `exp_variant_id`, `unit_type`, `agg_type`, `goal`, `dimension`, `dimension_value` and `unit_id`. See [`Experiment.evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for column descriptions and example result. \"\"\" pass","title":"Dao"},{"location":"api/dao.html#dao","text":"","title":"Dao"},{"location":"api/dao.html#epstats.toolkit.dao.Dao","text":"Abstract class interfacing any kind of underlying data source.","title":"epstats.toolkit.dao.Dao"},{"location":"api/dao.html#epstats.toolkit.dao.Dao.close","text":"Close underlying data source connection and frees resources (if any). Source code in epstats/toolkit/dao.py 31 32 33 34 35 def close ( self ) -> None : \"\"\" Close underlying data source connection and frees resources (if any). \"\"\" pass","title":"close()"},{"location":"api/dao.html#epstats.toolkit.dao.Dao.get_agg_goals","text":"Get goals data pre-aggregated by columns exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value . See [ Experiment.evaluate_agg ][epstats.toolkit.experiment.Experiment.evaluate_agg] for column descriptions and example result. Source code in epstats/toolkit/dao.py 21 22 23 24 25 26 27 28 29 def get_agg_goals ( self , experiment : Experiment ) -> pd . DataFrame : \"\"\" Get goals data pre-aggregated by columns `exp_variant_id`, `unit_type`, `agg_type`, `goal`, `dimension`, `dimension_value`. See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for column descriptions and example result. \"\"\" pass","title":"get_agg_goals()"},{"location":"api/dao.html#epstats.toolkit.dao.Dao.get_unit_goals","text":"Get goals data pre-aggregated by columns exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value and unit_id . See [ Experiment.evaluate_by_unit ][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for column descriptions and example result. Source code in epstats/toolkit/dao.py 11 12 13 14 15 16 17 18 19 def get_unit_goals ( self , experiment : Experiment ) -> pd . DataFrame : \"\"\" Get goals data pre-aggregated by columns `exp_variant_id`, `unit_type`, `agg_type`, `goal`, `dimension`, `dimension_value` and `unit_id`. See [`Experiment.evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for column descriptions and example result. \"\"\" pass","title":"get_unit_goals()"},{"location":"api/dao_factory.html","text":"DaoFactory \u00b6 \u00b6 Factory creating instances of [ Dao ][epstats.toolkit.dao.Dao] classes. It is used in API server to get dao for every request. get_dao ( self ) \u00b6 Create new instance of [ Dao ][epstats.toolkit.dao.Dao] to serve the request. Source code in epstats/toolkit/dao.py 45 46 47 48 49 def get_dao ( self ) -> Dao : \"\"\" Create new instance of [`Dao`][epstats.toolkit.dao.Dao] to serve the request. \"\"\" pass","title":"DaoFactory"},{"location":"api/dao_factory.html#daofactory","text":"","title":"DaoFactory"},{"location":"api/dao_factory.html#epstats.toolkit.dao.DaoFactory","text":"Factory creating instances of [ Dao ][epstats.toolkit.dao.Dao] classes. It is used in API server to get dao for every request.","title":"epstats.toolkit.dao.DaoFactory"},{"location":"api/dao_factory.html#epstats.toolkit.dao.DaoFactory.get_dao","text":"Create new instance of [ Dao ][epstats.toolkit.dao.Dao] to serve the request. Source code in epstats/toolkit/dao.py 45 46 47 48 49 def get_dao ( self ) -> Dao : \"\"\" Create new instance of [`Dao`][epstats.toolkit.dao.Dao] to serve the request. \"\"\" pass","title":"get_dao()"},{"location":"api/evaluation.html","text":"Evaluation \u00b6 \u00b6 Results of an experiment evaluation. check_columns () classmethod \u00b6 checks dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id check_id - check id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition variable_id - name of the variable in check evaluation, SRM check has following variables p_value , test_stat , confidence_level value - value of the variable Source code in epstats/toolkit/experiment.py 67 68 69 70 71 72 73 74 75 76 77 78 79 @classmethod def check_columns ( cls ) -> List [ str ]: \"\"\" `checks` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`, `test_stat`, `confidence_level` 1. `value` - value of the variable \"\"\" return [ 'timestamp' , 'exp_id' , 'check_id' , 'check_name' , 'variable_id' , 'value' ] exposure_columns () classmethod \u00b6 exposures dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id exp_variant_id - variant id exposures - number of exposures of this variant Source code in epstats/toolkit/experiment.py 81 82 83 84 85 86 87 88 89 90 91 @classmethod def exposure_columns ( cls ) -> List [ str ]: \"\"\" `exposures` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `exp_variant_id` - variant id 1. `exposures` - number of exposures of this variant \"\"\" return [ 'exp_variant_id' , 'exposures' ] metric_columns () classmethod \u00b6 metrics dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id metric_id - metric id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition metric_name - metric name as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition exp_variant_id - variant id count - number of exposures, value of metric denominator mean - sum_value / count std - sample standard deviation sum_value - value of goals, value of metric nominator confidence_level - current confidence level used to calculate p_value and confidence_interval diff - relative diff between sample means of this and control variant test_stat - value of test statistic of the relative difference in means p_value - p-value of the test statistic under current confidence_level confidence_interval - confidence interval of the diff under current confidence_level standard_error - standard error of the diff degrees_of_freedom - degrees of freedom of this variant mean Source code in epstats/toolkit/experiment.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @classmethod def metric_columns ( cls ) -> List [ str ]: \"\"\" `metrics` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `metric_id` - metric id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `metric_name` - metric name as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `exp_variant_id` - variant id 1. `count` - number of exposures, value of metric denominator 1. `mean` - `sum_value` / `count` 1. `std` - sample standard deviation 1. `sum_value` - value of goals, value of metric nominator 1. `confidence_level` - current confidence level used to calculate `p_value` and `confidence_interval` 1. `diff` - relative diff between sample means of this and control variant 1. `test_stat` - value of test statistic of the relative difference in means 1. `p_value` - p-value of the test statistic under current `confidence_level` 1. `confidence_interval` - confidence interval of the `diff` under current `confidence_level` 1. `standard_error` - standard error of the `diff` 1. `degrees_of_freedom` - degrees of freedom of this variant mean \"\"\" return [ 'timestamp' , 'exp_id' , 'metric_id' , 'metric_name' , 'exp_variant_id' , 'count' , 'mean' , 'std' , 'sum_value' , 'confidence_level' , 'diff' , 'test_stat' , 'p_value' , 'confidence_interval' , 'standard_error' , 'degrees_of_freedom' , ]","title":"Evaluation"},{"location":"api/evaluation.html#evaluation","text":"","title":"Evaluation"},{"location":"api/evaluation.html#epstats.toolkit.experiment.Evaluation","text":"Results of an experiment evaluation.","title":"epstats.toolkit.experiment.Evaluation"},{"location":"api/evaluation.html#epstats.toolkit.experiment.Evaluation.check_columns","text":"checks dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id check_id - check id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition variable_id - name of the variable in check evaluation, SRM check has following variables p_value , test_stat , confidence_level value - value of the variable Source code in epstats/toolkit/experiment.py 67 68 69 70 71 72 73 74 75 76 77 78 79 @classmethod def check_columns ( cls ) -> List [ str ]: \"\"\" `checks` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`, `test_stat`, `confidence_level` 1. `value` - value of the variable \"\"\" return [ 'timestamp' , 'exp_id' , 'check_id' , 'check_name' , 'variable_id' , 'value' ]","title":"check_columns()"},{"location":"api/evaluation.html#epstats.toolkit.experiment.Evaluation.exposure_columns","text":"exposures dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id exp_variant_id - variant id exposures - number of exposures of this variant Source code in epstats/toolkit/experiment.py 81 82 83 84 85 86 87 88 89 90 91 @classmethod def exposure_columns ( cls ) -> List [ str ]: \"\"\" `exposures` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `exp_variant_id` - variant id 1. `exposures` - number of exposures of this variant \"\"\" return [ 'exp_variant_id' , 'exposures' ]","title":"exposure_columns()"},{"location":"api/evaluation.html#epstats.toolkit.experiment.Evaluation.metric_columns","text":"metrics dataframe with columns: timestamp - timestamp of evaluation exp_id - experiment id metric_id - metric id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition metric_name - metric name as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition exp_variant_id - variant id count - number of exposures, value of metric denominator mean - sum_value / count std - sample standard deviation sum_value - value of goals, value of metric nominator confidence_level - current confidence level used to calculate p_value and confidence_interval diff - relative diff between sample means of this and control variant test_stat - value of test statistic of the relative difference in means p_value - p-value of the test statistic under current confidence_level confidence_interval - confidence interval of the diff under current confidence_level standard_error - standard error of the diff degrees_of_freedom - degrees of freedom of this variant mean Source code in epstats/toolkit/experiment.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @classmethod def metric_columns ( cls ) -> List [ str ]: \"\"\" `metrics` dataframe with columns: 1. `timestamp` - timestamp of evaluation 1. `exp_id` - experiment id 1. `metric_id` - metric id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `metric_name` - metric name as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `exp_variant_id` - variant id 1. `count` - number of exposures, value of metric denominator 1. `mean` - `sum_value` / `count` 1. `std` - sample standard deviation 1. `sum_value` - value of goals, value of metric nominator 1. `confidence_level` - current confidence level used to calculate `p_value` and `confidence_interval` 1. `diff` - relative diff between sample means of this and control variant 1. `test_stat` - value of test statistic of the relative difference in means 1. `p_value` - p-value of the test statistic under current `confidence_level` 1. `confidence_interval` - confidence interval of the `diff` under current `confidence_level` 1. `standard_error` - standard error of the `diff` 1. `degrees_of_freedom` - degrees of freedom of this variant mean \"\"\" return [ 'timestamp' , 'exp_id' , 'metric_id' , 'metric_name' , 'exp_variant_id' , 'count' , 'mean' , 'std' , 'sum_value' , 'confidence_level' , 'diff' , 'test_stat' , 'p_value' , 'confidence_interval' , 'standard_error' , 'degrees_of_freedom' , ]","title":"metric_columns()"},{"location":"api/experiment.html","text":"Experiment \u00b6 \u00b6 Evaluate one experiment described as a list of metrics and checks. See Statistics for details about statistical method used and [ Evaluation ][epstats.toolkit.experiment.Evaluation] for description of output. evaluate_agg ( self , goals ) \u00b6 Evaluate all metrics and checks in the experiment from already pre-aggregated goals. This method is usefull when there are too many units in the experiment to evaluate it using [ evaluate_by_unit ][epstats.toolkit.experiment.Experiment.evaluate_by_unit]. Does best effort to fill in missing goals and variants with zeros. Parameters: Name Type Description Default goals DataFrame dataframe with one row per goal and aggregated data in columns required goals dataframe columns: exp_id - experiment id exp_variant_id - variant unit_type - randomization unit type agg_type - level of aggregation goal - goal name dimension - name of the dimension, e.g. product dimension_value - value of the dimension, e.g. p_1 count - number of observed goals (e.g. conversions) sum_sqr_count - summed squared number of observed goals per unit, it is similar to sum_sqr_value sum_value - value of observed goals sum_sqr_value - summed squared value per unit. This is used to calculate sample standard deviation from pre-aggregated data (it is a term \\sum x^2 \\sum x^2 in \\hat{\\sigma}^2 = \\frac{\\sum x^2 - \\frac{(\\sum x)^2}{n}}{n-1} \\hat{\\sigma}^2 = \\frac{\\sum x^2 - \\frac{(\\sum x)^2}{n}}{n-1} ). count_unique - number of units with at least 1 observed goal Returns: Type Description Evaluation set of dataframes with evaluation Usage: from epstats.toolkit import Experiment , Metric , SrmCheck experiment = Experiment ( 'test-conversion' , 'a' , [ Metric ( 1 , 'Click-through Rate' , 'count(test_unit_type.unit.click)' , 'count(test_unit_type.global.exposure)' ), ], [ SrmCheck ( 1 , 'SRM' , 'count(test_unit_type.global.exposure)' )], unit_type = 'test_unit_type' ) # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData . load_goals_agg ( experiment . id ) # evaluate experiment ev = experiment . evaluate_agg ( goals ) # work with results print ( ev . exposures ) print ( ev . metrics [ ev . metrics == 1 ]) print ( ev . checks [ ev . checks == 1 ]) # this is to assert that this code sample works correctly from epstats.toolkit.testing import TestDao assert_experiment ( experiment , ev , TestDao ( TestData ())) Input data frame example: exp_id exp_variant_id unit_type agg_type goal dimension dimension_value count sum_sqr_count sum_value sum_sqr_value count_unique test-srm a test_unit_type global exposure 100000 100000 100000 100000 100000 test-srm b test_unit_type global exposure 100100 100100 100100 100100 100100 test-srm a test_unit_type unit conversion 1200 1800 32000 66528 900 test-srm a test_unit_type_2 global conversion product product_1 1000 1700 31000 55000 850 Source code in epstats/toolkit/experiment.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def evaluate_agg ( self , goals : pd . DataFrame ) -> Evaluation : \"\"\" Evaluate all metrics and checks in the experiment from already pre-aggregated goals. This method is usefull when there are too many units in the experiment to evaluate it using [`evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit]. Does best effort to fill in missing goals and variants with zeros. Arguments: goals: dataframe with one row per goal and aggregated data in columns `goals` dataframe columns: 1. `exp_id` - experiment id 1. `exp_variant_id` - variant 1. `unit_type` - randomization unit type 1. `agg_type` - level of aggregation 1. `goal` - goal name 1. `dimension` - name of the dimension, e.g. `product` 1. `dimension_value` - value of the dimension, e.g. `p_1` 1. `count` - number of observed goals (e.g. conversions) 1. `sum_sqr_count` - summed squared number of observed goals per unit, it is similar to `sum_sqr_value` 1. `sum_value` - value of observed goals 1. `sum_sqr_value` - summed squared value per unit. This is used to calculate sample standard deviation from pre-aggregated data (it is a term $\\sum x^2$ in $\\hat{\\sigma}^2 = \\\\frac{\\sum x^2 - \\\\frac{(\\sum x)^2}{n}}{n-1}$). 1. `count_unique` - number of units with at least 1 observed goal Returns: set of dataframes with evaluation Usage: ```python from epstats.toolkit import Experiment, Metric, SrmCheck experiment = Experiment( 'test-conversion', 'a', [Metric( 1, 'Click-through Rate', 'count(test_unit_type.unit.click)', 'count(test_unit_type.global.exposure)'), ], [SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')], unit_type='test_unit_type') # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData.load_goals_agg(experiment.id) # evaluate experiment ev = experiment.evaluate_agg(goals) # work with results print(ev.exposures) print(ev.metrics[ev.metrics == 1]) print(ev.checks[ev.checks == 1]) # this is to assert that this code sample works correctly from epstats.toolkit.testing import TestDao assert_experiment(experiment, ev, TestDao(TestData())) ``` Input data frame example: ``` exp_id exp_variant_id unit_type agg_type goal dimension dimension_value count sum_sqr_count sum_value sum_sqr_value count_unique test-srm a test_unit_type global exposure 100000 100000 100000 100000 100000 test-srm b test_unit_type global exposure 100100 100100 100100 100100 100100 test-srm a test_unit_type unit conversion 1200 1800 32000 66528 900 test-srm a test_unit_type_2 global conversion product product_1 1000 1700 31000 55000 850 ``` \"\"\" g = self . _fix_missing_agg ( goals ) return self . _evaluate ( g , Experiment . _metrics_column_fce_agg , Experiment . _checks_fce_agg , Experiment . _exposures_fce_agg , ) evaluate_by_unit ( self , goals ) \u00b6 Evaluate all metrics and checks in the experiment from goals grouped by unit_id . This method is usefull when there are not many (<1M) units in the experiment to evaluate it. If there many units exposed to the experiment, pre-aggregate data and use [ evaluate_agg ][epstats.toolkit.experiment.Experiment.evaluate_agg]. Does best effort to fill in missing goals and variants with zeros. Parameters: Name Type Description Default goals DataFrame dataframe with one row per goal and aggregated data in columns required goals dataframe columns: exp_id - experiment id exp_variant_id - variant unit_type - randomization unit type unit_id - (randomization) unit id agg_type - level of aggregation goal - goal name dimension - name of the dimension, e.g. product dimension_value - value of the dimension, e.g. p_1 count - number of observed goals sum_value - value of observed goals Returns: Type Description Evaluation set of dataframes with evaluation Usage: from epstats.toolkit import Experiment , Metric , SrmCheck experiment = Experiment ( 'test-real-valued' , 'a' , [ Metric ( 2 , 'Average Bookings' , 'value(test_unit_type.unit.conversion)' , 'count(test_unit_type.unit.exposure)' ) ], [], unit_type = 'test_unit_type' ) # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData . load_goals_by_unit ( experiment . id ) # evaluate experiment ev = experiment . evaluate_by_unit ( goals ) # work with results print ( ev . exposures ) print ( ev . metrics [ ev . metrics == 1 ]) print ( ev . checks [ ev . checks == 1 ]) # this is to assert that this code sample works correctly from epstats.toolkit.testing import TestDao assert_experiment ( experiment , ev , TestDao ( TestData ())) Input data frame example: exp_id exp_variant_id unit_type unit_id agg_type goal dimension dimension_value count sum_value test-srm a test_unit_type test_unit_type_1 unit exposure 1 1 test-srm a test_unit_type test_unit_type_1 unit conversion product product_1 2 75 test-srm b test_unit_type test_unit_type_2 unit exposure 1 1 test-srm b test_unit_type test_unit_type_3 unit exposure 1 1 test-srm b test_unit_type test_unit_type_3 unit conversion product product_2 1 1 Source code in epstats/toolkit/experiment.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def evaluate_by_unit ( self , goals : pd . DataFrame ) -> Evaluation : \"\"\" Evaluate all metrics and checks in the experiment from goals grouped by `unit_id`. This method is usefull when there are not many (<1M) units in the experiment to evaluate it. If there many units exposed to the experiment, pre-aggregate data and use [`evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg]. Does best effort to fill in missing goals and variants with zeros. Arguments: goals: dataframe with one row per goal and aggregated data in columns `goals` dataframe columns: 1. `exp_id` - experiment id 1. `exp_variant_id` - variant 1. `unit_type` - randomization unit type 1. `unit_id` - (randomization) unit id 1. `agg_type` - level of aggregation 1. `goal` - goal name 1. `dimension` - name of the dimension, e.g. `product` 1. `dimension_value` - value of the dimension, e.g. `p_1` 1. `count` - number of observed goals 1. `sum_value` - value of observed goals Returns: set of dataframes with evaluation Usage: ```python from epstats.toolkit import Experiment, Metric, SrmCheck experiment = Experiment( 'test-real-valued', 'a', [Metric( 2, 'Average Bookings', 'value(test_unit_type.unit.conversion)', 'count(test_unit_type.unit.exposure)') ], [], unit_type='test_unit_type') # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData.load_goals_by_unit(experiment.id) # evaluate experiment ev = experiment.evaluate_by_unit(goals) # work with results print(ev.exposures) print(ev.metrics[ev.metrics == 1]) print(ev.checks[ev.checks == 1]) # this is to assert that this code sample works correctly from epstats.toolkit.testing import TestDao assert_experiment(experiment, ev, TestDao(TestData())) ``` Input data frame example: ``` exp_id exp_variant_id unit_type unit_id agg_type goal dimension dimension_value count sum_value test-srm a test_unit_type test_unit_type_1 unit exposure 1 1 test-srm a test_unit_type test_unit_type_1 unit conversion product product_1 2 75 test-srm b test_unit_type test_unit_type_2 unit exposure 1 1 test-srm b test_unit_type test_unit_type_3 unit exposure 1 1 test-srm b test_unit_type test_unit_type_3 unit conversion product product_2 1 1 ``` \"\"\" g = self . _fix_missing_by_unit ( goals ) # We need to pivot table to get all goals per `unit_id` on the same row in the data frame. # This is needed to be able to vector-evaluate compound metrics # eg. `value(test_unit_type.unit.conversion) - value(test_unit_type.unit.refund)` g = ( pd . pivot_table ( g , values = [ 'count' , 'sum_value' ], index = [ 'exp_id' , 'exp_variant_id' , 'unit_type' , 'agg_type' , 'unit_id' , 'dimension' , 'dimension_value' , ], columns = 'goal' , aggfunc = np . sum , fill_value = 0 , ) . swaplevel ( axis = 1 ) . reset_index () ) return self . _evaluate ( g , Experiment . _metrics_column_fce_by_unit , Experiment . _checks_fce_by_unit , Experiment . _exposures_fce_by_unit , ) get_goals ( self ) \u00b6 List of all goals needed to evaluate all metrics and checks in the experiment. Returns: Type Description List[epstats.toolkit.parser.EpGoal] list of parsed structured goals Source code in epstats/toolkit/experiment.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def get_goals ( self ) -> List [ EpGoal ]: \"\"\" List of all goals needed to evaluate all metrics and checks in the experiment. Returns: list of parsed structured goals \"\"\" res = set () for m in self . metrics : res = res . union ( m . get_goals ()) for c in self . checks : res = res . union ( c . get_goals ()) res = res . union ( self . _exposure_goals ) return list ( res )","title":"Experiment"},{"location":"api/experiment.html#experiment","text":"","title":"Experiment"},{"location":"api/experiment.html#epstats.toolkit.experiment.Experiment","text":"Evaluate one experiment described as a list of metrics and checks. See Statistics for details about statistical method used and [ Evaluation ][epstats.toolkit.experiment.Evaluation] for description of output.","title":"epstats.toolkit.experiment.Experiment"},{"location":"api/experiment.html#epstats.toolkit.experiment.Experiment.evaluate_agg","text":"Evaluate all metrics and checks in the experiment from already pre-aggregated goals. This method is usefull when there are too many units in the experiment to evaluate it using [ evaluate_by_unit ][epstats.toolkit.experiment.Experiment.evaluate_by_unit]. Does best effort to fill in missing goals and variants with zeros. Parameters: Name Type Description Default goals DataFrame dataframe with one row per goal and aggregated data in columns required goals dataframe columns: exp_id - experiment id exp_variant_id - variant unit_type - randomization unit type agg_type - level of aggregation goal - goal name dimension - name of the dimension, e.g. product dimension_value - value of the dimension, e.g. p_1 count - number of observed goals (e.g. conversions) sum_sqr_count - summed squared number of observed goals per unit, it is similar to sum_sqr_value sum_value - value of observed goals sum_sqr_value - summed squared value per unit. This is used to calculate sample standard deviation from pre-aggregated data (it is a term \\sum x^2 \\sum x^2 in \\hat{\\sigma}^2 = \\frac{\\sum x^2 - \\frac{(\\sum x)^2}{n}}{n-1} \\hat{\\sigma}^2 = \\frac{\\sum x^2 - \\frac{(\\sum x)^2}{n}}{n-1} ). count_unique - number of units with at least 1 observed goal Returns: Type Description Evaluation set of dataframes with evaluation Usage: from epstats.toolkit import Experiment , Metric , SrmCheck experiment = Experiment ( 'test-conversion' , 'a' , [ Metric ( 1 , 'Click-through Rate' , 'count(test_unit_type.unit.click)' , 'count(test_unit_type.global.exposure)' ), ], [ SrmCheck ( 1 , 'SRM' , 'count(test_unit_type.global.exposure)' )], unit_type = 'test_unit_type' ) # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData . load_goals_agg ( experiment . id ) # evaluate experiment ev = experiment . evaluate_agg ( goals ) # work with results print ( ev . exposures ) print ( ev . metrics [ ev . metrics == 1 ]) print ( ev . checks [ ev . checks == 1 ]) # this is to assert that this code sample works correctly from epstats.toolkit.testing import TestDao assert_experiment ( experiment , ev , TestDao ( TestData ())) Input data frame example: exp_id exp_variant_id unit_type agg_type goal dimension dimension_value count sum_sqr_count sum_value sum_sqr_value count_unique test-srm a test_unit_type global exposure 100000 100000 100000 100000 100000 test-srm b test_unit_type global exposure 100100 100100 100100 100100 100100 test-srm a test_unit_type unit conversion 1200 1800 32000 66528 900 test-srm a test_unit_type_2 global conversion product product_1 1000 1700 31000 55000 850 Source code in epstats/toolkit/experiment.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def evaluate_agg ( self , goals : pd . DataFrame ) -> Evaluation : \"\"\" Evaluate all metrics and checks in the experiment from already pre-aggregated goals. This method is usefull when there are too many units in the experiment to evaluate it using [`evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit]. Does best effort to fill in missing goals and variants with zeros. Arguments: goals: dataframe with one row per goal and aggregated data in columns `goals` dataframe columns: 1. `exp_id` - experiment id 1. `exp_variant_id` - variant 1. `unit_type` - randomization unit type 1. `agg_type` - level of aggregation 1. `goal` - goal name 1. `dimension` - name of the dimension, e.g. `product` 1. `dimension_value` - value of the dimension, e.g. `p_1` 1. `count` - number of observed goals (e.g. conversions) 1. `sum_sqr_count` - summed squared number of observed goals per unit, it is similar to `sum_sqr_value` 1. `sum_value` - value of observed goals 1. `sum_sqr_value` - summed squared value per unit. This is used to calculate sample standard deviation from pre-aggregated data (it is a term $\\sum x^2$ in $\\hat{\\sigma}^2 = \\\\frac{\\sum x^2 - \\\\frac{(\\sum x)^2}{n}}{n-1}$). 1. `count_unique` - number of units with at least 1 observed goal Returns: set of dataframes with evaluation Usage: ```python from epstats.toolkit import Experiment, Metric, SrmCheck experiment = Experiment( 'test-conversion', 'a', [Metric( 1, 'Click-through Rate', 'count(test_unit_type.unit.click)', 'count(test_unit_type.global.exposure)'), ], [SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')], unit_type='test_unit_type') # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData.load_goals_agg(experiment.id) # evaluate experiment ev = experiment.evaluate_agg(goals) # work with results print(ev.exposures) print(ev.metrics[ev.metrics == 1]) print(ev.checks[ev.checks == 1]) # this is to assert that this code sample works correctly from epstats.toolkit.testing import TestDao assert_experiment(experiment, ev, TestDao(TestData())) ``` Input data frame example: ``` exp_id exp_variant_id unit_type agg_type goal dimension dimension_value count sum_sqr_count sum_value sum_sqr_value count_unique test-srm a test_unit_type global exposure 100000 100000 100000 100000 100000 test-srm b test_unit_type global exposure 100100 100100 100100 100100 100100 test-srm a test_unit_type unit conversion 1200 1800 32000 66528 900 test-srm a test_unit_type_2 global conversion product product_1 1000 1700 31000 55000 850 ``` \"\"\" g = self . _fix_missing_agg ( goals ) return self . _evaluate ( g , Experiment . _metrics_column_fce_agg , Experiment . _checks_fce_agg , Experiment . _exposures_fce_agg , )","title":"evaluate_agg()"},{"location":"api/experiment.html#epstats.toolkit.experiment.Experiment.evaluate_by_unit","text":"Evaluate all metrics and checks in the experiment from goals grouped by unit_id . This method is usefull when there are not many (<1M) units in the experiment to evaluate it. If there many units exposed to the experiment, pre-aggregate data and use [ evaluate_agg ][epstats.toolkit.experiment.Experiment.evaluate_agg]. Does best effort to fill in missing goals and variants with zeros. Parameters: Name Type Description Default goals DataFrame dataframe with one row per goal and aggregated data in columns required goals dataframe columns: exp_id - experiment id exp_variant_id - variant unit_type - randomization unit type unit_id - (randomization) unit id agg_type - level of aggregation goal - goal name dimension - name of the dimension, e.g. product dimension_value - value of the dimension, e.g. p_1 count - number of observed goals sum_value - value of observed goals Returns: Type Description Evaluation set of dataframes with evaluation Usage: from epstats.toolkit import Experiment , Metric , SrmCheck experiment = Experiment ( 'test-real-valued' , 'a' , [ Metric ( 2 , 'Average Bookings' , 'value(test_unit_type.unit.conversion)' , 'count(test_unit_type.unit.exposure)' ) ], [], unit_type = 'test_unit_type' ) # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData . load_goals_by_unit ( experiment . id ) # evaluate experiment ev = experiment . evaluate_by_unit ( goals ) # work with results print ( ev . exposures ) print ( ev . metrics [ ev . metrics == 1 ]) print ( ev . checks [ ev . checks == 1 ]) # this is to assert that this code sample works correctly from epstats.toolkit.testing import TestDao assert_experiment ( experiment , ev , TestDao ( TestData ())) Input data frame example: exp_id exp_variant_id unit_type unit_id agg_type goal dimension dimension_value count sum_value test-srm a test_unit_type test_unit_type_1 unit exposure 1 1 test-srm a test_unit_type test_unit_type_1 unit conversion product product_1 2 75 test-srm b test_unit_type test_unit_type_2 unit exposure 1 1 test-srm b test_unit_type test_unit_type_3 unit exposure 1 1 test-srm b test_unit_type test_unit_type_3 unit conversion product product_2 1 1 Source code in epstats/toolkit/experiment.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def evaluate_by_unit ( self , goals : pd . DataFrame ) -> Evaluation : \"\"\" Evaluate all metrics and checks in the experiment from goals grouped by `unit_id`. This method is usefull when there are not many (<1M) units in the experiment to evaluate it. If there many units exposed to the experiment, pre-aggregate data and use [`evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg]. Does best effort to fill in missing goals and variants with zeros. Arguments: goals: dataframe with one row per goal and aggregated data in columns `goals` dataframe columns: 1. `exp_id` - experiment id 1. `exp_variant_id` - variant 1. `unit_type` - randomization unit type 1. `unit_id` - (randomization) unit id 1. `agg_type` - level of aggregation 1. `goal` - goal name 1. `dimension` - name of the dimension, e.g. `product` 1. `dimension_value` - value of the dimension, e.g. `p_1` 1. `count` - number of observed goals 1. `sum_value` - value of observed goals Returns: set of dataframes with evaluation Usage: ```python from epstats.toolkit import Experiment, Metric, SrmCheck experiment = Experiment( 'test-real-valued', 'a', [Metric( 2, 'Average Bookings', 'value(test_unit_type.unit.conversion)', 'count(test_unit_type.unit.exposure)') ], [], unit_type='test_unit_type') # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData.load_goals_by_unit(experiment.id) # evaluate experiment ev = experiment.evaluate_by_unit(goals) # work with results print(ev.exposures) print(ev.metrics[ev.metrics == 1]) print(ev.checks[ev.checks == 1]) # this is to assert that this code sample works correctly from epstats.toolkit.testing import TestDao assert_experiment(experiment, ev, TestDao(TestData())) ``` Input data frame example: ``` exp_id exp_variant_id unit_type unit_id agg_type goal dimension dimension_value count sum_value test-srm a test_unit_type test_unit_type_1 unit exposure 1 1 test-srm a test_unit_type test_unit_type_1 unit conversion product product_1 2 75 test-srm b test_unit_type test_unit_type_2 unit exposure 1 1 test-srm b test_unit_type test_unit_type_3 unit exposure 1 1 test-srm b test_unit_type test_unit_type_3 unit conversion product product_2 1 1 ``` \"\"\" g = self . _fix_missing_by_unit ( goals ) # We need to pivot table to get all goals per `unit_id` on the same row in the data frame. # This is needed to be able to vector-evaluate compound metrics # eg. `value(test_unit_type.unit.conversion) - value(test_unit_type.unit.refund)` g = ( pd . pivot_table ( g , values = [ 'count' , 'sum_value' ], index = [ 'exp_id' , 'exp_variant_id' , 'unit_type' , 'agg_type' , 'unit_id' , 'dimension' , 'dimension_value' , ], columns = 'goal' , aggfunc = np . sum , fill_value = 0 , ) . swaplevel ( axis = 1 ) . reset_index () ) return self . _evaluate ( g , Experiment . _metrics_column_fce_by_unit , Experiment . _checks_fce_by_unit , Experiment . _exposures_fce_by_unit , )","title":"evaluate_by_unit()"},{"location":"api/experiment.html#epstats.toolkit.experiment.Experiment.get_goals","text":"List of all goals needed to evaluate all metrics and checks in the experiment. Returns: Type Description List[epstats.toolkit.parser.EpGoal] list of parsed structured goals Source code in epstats/toolkit/experiment.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def get_goals ( self ) -> List [ EpGoal ]: \"\"\" List of all goals needed to evaluate all metrics and checks in the experiment. Returns: list of parsed structured goals \"\"\" res = set () for m in self . metrics : res = res . union ( m . get_goals ()) for c in self . checks : res = res . union ( c . get_goals ()) res = res . union ( self . _exposure_goals ) return list ( res )","title":"get_goals()"},{"location":"api/srm_check.html","text":"SrmCheck \u00b6 \u00b6 Sample ratio mismatch check checking randomization of units to variants using Chi-square test . evaluate_agg ( self , goals , default_exp_variant_id ) \u00b6 See [ Check.evaluate_agg ][epstats.toolkit.check.Check.evaluate_agg]. Source code in epstats/toolkit/check.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def evaluate_agg ( self , goals : pd . DataFrame , default_exp_variant_id : str ) -> pd . DataFrame : \"\"\" See [`Check.evaluate_agg`][epstats.toolkit.check.Check.evaluate_agg]. \"\"\" # input example: # test - srm, a, global.exposure, 10000, 10010, 10010, 0.0, 0.0 # test - srm, b, global.exposure, 10010, 10010, 10010, 0.0, 0.0 # test - srm, c, global.exposure, 10040, 10040, 10040, 0.0, 0.0 # output example: # test - srm, 1, SRM, p_value, 0.20438 # test - srm, 1, SRM, test_stat, 3.17552 # test - srm, 1, SRM, confidence_level, 0.999 # prepare data - we only need exposures exposures , _ , _ = self . _parser . evaluate_agg ( goals ) # chi-square test with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): # we fill in zeros, when goal data are missing for some variant. # There could be division by zero here which is expected as we return # nan or inf values to the caller. stat , pval = chisquare ( exposures ) r = pd . DataFrame ( { 'check_id' : [ self . id , self . id , self . id ], 'check_name' : [ self . name , self . name , self . name ], 'variable_id' : [ 'p_value' , 'test_stat' , 'confidence_level' ], 'value' : [ pval , stat , self . confidence_level ], } ) return r evaluate_by_unit ( self , goals , default_exp_variant_id ) \u00b6 See [ Check.evaluate_by_unit ][epstats.toolkit.check.Check.evaluate_by_unit]. Source code in epstats/toolkit/check.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def evaluate_by_unit ( self , goals : pd . DataFrame , default_exp_variant_id : str ) -> pd . DataFrame : \"\"\" See [`Check.evaluate_by_unit`][epstats.toolkit.check.Check.evaluate_by_unit]. \"\"\" exposures , _ , _ = self . _parser . evaluate_by_unit ( goals ) # chi-square test stat , pval = chisquare ( exposures ) r = pd . DataFrame ( { 'check_id' : [ self . id , self . id , self . id ], 'check_name' : [ self . name , self . name , self . name ], 'variable_id' : [ 'p_value' , 'test_stat' , 'confidence_level' ], 'value' : [ pval , stat , self . confidence_level ], } ) return r","title":"SrmCheck"},{"location":"api/srm_check.html#srmcheck","text":"","title":"SrmCheck"},{"location":"api/srm_check.html#epstats.toolkit.check.SrmCheck","text":"Sample ratio mismatch check checking randomization of units to variants using Chi-square test .","title":"epstats.toolkit.check.SrmCheck"},{"location":"api/srm_check.html#epstats.toolkit.check.SrmCheck.evaluate_agg","text":"See [ Check.evaluate_agg ][epstats.toolkit.check.Check.evaluate_agg]. Source code in epstats/toolkit/check.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def evaluate_agg ( self , goals : pd . DataFrame , default_exp_variant_id : str ) -> pd . DataFrame : \"\"\" See [`Check.evaluate_agg`][epstats.toolkit.check.Check.evaluate_agg]. \"\"\" # input example: # test - srm, a, global.exposure, 10000, 10010, 10010, 0.0, 0.0 # test - srm, b, global.exposure, 10010, 10010, 10010, 0.0, 0.0 # test - srm, c, global.exposure, 10040, 10040, 10040, 0.0, 0.0 # output example: # test - srm, 1, SRM, p_value, 0.20438 # test - srm, 1, SRM, test_stat, 3.17552 # test - srm, 1, SRM, confidence_level, 0.999 # prepare data - we only need exposures exposures , _ , _ = self . _parser . evaluate_agg ( goals ) # chi-square test with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): # we fill in zeros, when goal data are missing for some variant. # There could be division by zero here which is expected as we return # nan or inf values to the caller. stat , pval = chisquare ( exposures ) r = pd . DataFrame ( { 'check_id' : [ self . id , self . id , self . id ], 'check_name' : [ self . name , self . name , self . name ], 'variable_id' : [ 'p_value' , 'test_stat' , 'confidence_level' ], 'value' : [ pval , stat , self . confidence_level ], } ) return r","title":"evaluate_agg()"},{"location":"api/srm_check.html#epstats.toolkit.check.SrmCheck.evaluate_by_unit","text":"See [ Check.evaluate_by_unit ][epstats.toolkit.check.Check.evaluate_by_unit]. Source code in epstats/toolkit/check.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def evaluate_by_unit ( self , goals : pd . DataFrame , default_exp_variant_id : str ) -> pd . DataFrame : \"\"\" See [`Check.evaluate_by_unit`][epstats.toolkit.check.Check.evaluate_by_unit]. \"\"\" exposures , _ , _ = self . _parser . evaluate_by_unit ( goals ) # chi-square test stat , pval = chisquare ( exposures ) r = pd . DataFrame ( { 'check_id' : [ self . id , self . id , self . id ], 'check_name' : [ self . name , self . name , self . name ], 'variable_id' : [ 'p_value' , 'test_stat' , 'confidence_level' ], 'value' : [ pval , stat , self . confidence_level ], } ) return r","title":"evaluate_by_unit()"},{"location":"api/statistics.html","text":"Statistics \u00b6 \u00b6 Various methods needed to evaluate experiment. multiple_comparisons_correction ( df , variants , metrics , confidence_level ) classmethod \u00b6 Holm-Bonferroni correction for multiple comparisons problem. It is applied when we have more than two variants, i.e. we have one control variant and at least two treatment variants. It adjusts p-value and length of confidence interval - both to be more conservative. Complete manual Algorithm: For each metric, select (unadjusted) p-values and replace them with adjusted ones. Based on adjustment ratio, compute new (adjusted) confidence intervals and replace old (unadjusted) ones. Parameters: Name Type Description Default df DataFrame dataframe as output of [ ttest_evaluation ][epstats.toolkit.statistics.Statistics.ttest_evaluation] required variants int number of variants in the experiment required metrics int number of metrics of experiment required confidence_level float desired confidence level at the end of the experiment, e.g. 0.95 required Returns: Type Description DataFrame dataframe of the same format as input with adjusted p-values and confidence intervals. Source code in epstats/toolkit/statistics.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 @classmethod def multiple_comparisons_correction ( cls , df : pd . DataFrame , variants : int , metrics : int , confidence_level : float ) -> pd . DataFrame : \"\"\" [Holm-Bonferroni correction](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method) for multiple comparisons problem. It is applied when we have more than two variants, i.e. we have one control variant and at least two treatment variants. It adjusts p-value and length of confidence interval - both to be more conservative. [Complete manual](../stats/multiple.md) Algorithm: For each metric, select (unadjusted) p-values and replace them with adjusted ones. Based on adjustment ratio, compute new (adjusted) confidence intervals and replace old (unadjusted) ones. Arguments: df: dataframe as output of [`ttest_evaluation`][epstats.toolkit.statistics.Statistics.ttest_evaluation] variants: number of variants in the experiment metrics: number of metrics of experiment confidence_level: desired confidence level at the end of the experiment, e.g. 0.95 Returns: dataframe of the same format as input with adjusted p-values and confidence intervals. \"\"\" alpha = 1 - confidence_level # level of significance for m in range ( metrics ): # indices of rows with metric m data index_from = m * variants + 1 index_to = ( m + 1 ) * variants - 1 # p-value adjustment pvals = df . loc [ index_from : index_to , 'p_value' ] . to_list () # select old p-values adj_pvals = multipletests ( pvals = pvals , alpha = alpha , method = 'holm' )[ 1 ] # compute adjusted p-values # confidence interval adjustment # we set ratio to 1 when test_stat is so big that pvals are zero, no reason to update ci adj_ratio = np . nan_to_num ( pvals / adj_pvals , nan = 1 ) # adjustment ratio adj_alpha = adj_ratio * alpha # adjusted level alpha f = df . loc [ index_from : index_to , 'degrees_of_freedom' ] . to_list () # degrees of freedom se = df . loc [ index_from : index_to , 'standard_error' ] . to_list () # standard error t_quantile = st . t . ppf ( np . ones ( variants - 1 ) - adj_alpha + adj_alpha / 2 , f ) # right t-quantile adj_conf_int = se * t_quantile # adjusted confidence interval # replace (unadjusted) p-values and confidence intervals with new adjusted ones df . loc [ index_from : index_to , 'p_value' ] = adj_pvals df . loc [ index_from : index_to , 'confidence_interval' ] = adj_conf_int return df obf_alpha_spending_function ( confidence_level , total_length , actual_day ) classmethod \u00b6 O'Brien-Fleming alpha spending function . We adjust confidence level in time in experiment. Confidence level in this setting is a decreasing function of experiment time. See Sequential Analysis for details. Parameters: Name Type Description Default confidence_level int required confidence level at the end of the test, e.g. 0.95 required total_length int length of the test in days, e.g. 7, 14, 21 required actual_day int actual days in the experiment period, must be between 1 and total_length required Returns: Type Description int adjusted confidence level with respect to actual day of the experiment and total length of the experiment. Source code in epstats/toolkit/statistics.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 @classmethod def obf_alpha_spending_function ( cls , confidence_level : int , total_length : int , actual_day : int ) -> int : \"\"\" [O'Brien-Fleming alpha spending function](https://online.stat.psu.edu/stat509/node/80/). We adjust confidence level in time in experiment. Confidence level in this setting is a decreasing function of experiment time. See [Sequential Analysis](../stats/sequential.md) for details. Arguments: confidence_level: required confidence level at the end of the test, e.g. 0.95 total_length: length of the test in days, e.g. 7, 14, 21 actual_day: actual days in the experiment period, must be between 1 and `total_length` Returns: adjusted confidence level with respect to actual day of the experiment and total length of the experiment. \"\"\" alpha = 1 - confidence_level t = actual_day / total_length # t in (0, 1] q = st . norm . ppf ( 1 - alpha / 2 ) # quantile of normal distribution alpha_adj = 2 - 2 * st . norm . cdf ( q / np . sqrt ( t )) return 1 - alpha_adj ttest_evaluation ( stats ) classmethod \u00b6 Testing statistical significance of relative difference in means of treatment and control variant. This is inspired by scipy.stats.ttest_ind_from_stats method that returns many more statistics than p-value and test statistic. Statistics used: Welch's t-test Welch\u2013Satterthwaite equation approximation of degrees of freedom. Parameters: Name Type Description Default stats <built-in function array> array with dimensions (metrics, variants, stats) required stats array values: metric_id metric_name exp_variant_id count mean std sum_value sum_sqr_value Returns: Type Description DataFrame dataframe containing statistics from the t-test Schema of returned dataframe: metric_id - metric id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition metric_name - metric name as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition exp_variant_id - variant id count - number of exposures, value of metric denominator mean - sum_value / count std - sample standard deviation sum_value - value of goals, value of metric nominator confidence_level - current confidence level used to calculate p_value and confidence_interval diff - relative diff between sample means of this and control variant test_stat - value of test statistic of the relative difference in means p_value - p-value of the test statistic under current confidence_level confidence_interval - confidence interval of the diff under current confidence_level standard_error - standard error of the diff degrees_of_freedom - degrees of freedom of this variant mean Source code in epstats/toolkit/statistics.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @classmethod def ttest_evaluation ( cls , stats : np . array ) -> pd . DataFrame : \"\"\" Testing statistical significance of relative difference in means of treatment and control variant. This is inspired by [scipy.stats.ttest_ind_from_stats](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html) method that returns many more statistics than p-value and test statistic. Statistics used: 1. [Welch's t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test) 1. [Welch\u2013Satterthwaite equation](https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation) approximation of degrees of freedom. Arguments: stats: array with dimensions (metrics, variants, stats) `stats` array values: 0. `metric_id` 1. `metric_name` 1. `exp_variant_id` 1. `count` 1. `mean` 1. `std` 1. `sum_value` 1. `sum_sqr_value` Returns: dataframe containing statistics from the t-test Schema of returned dataframe: 1. `metric_id` - metric id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `metric_name` - metric name as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `exp_variant_id` - variant id 1. `count` - number of exposures, value of metric denominator 1. `mean` - `sum_value` / `count` 1. `std` - sample standard deviation 1. `sum_value` - value of goals, value of metric nominator 1. `confidence_level` - current confidence level used to calculate `p_value` and `confidence_interval` 1. `diff` - relative diff between sample means of this and control variant 1. `test_stat` - value of test statistic of the relative difference in means 1. `p_value` - p-value of the test statistic under current `confidence_level` 1. `confidence_interval` - confidence interval of the `diff` under current `confidence_level` 1. `standard_error` - standard error of the `diff` 1. `degrees_of_freedom` - degrees of freedom of this variant mean \"\"\" stats = stats . transpose ( 1 , 2 , 0 ) stat_res = [] # semiresults variants_count = stats . shape [ 0 ] # number of variants # get only stats (not metric_id, metric_name, exp_variant_id) from the stats array as floats stats_values = stats [:, 3 : 8 , :] . astype ( float ) # control variant values s = stats_values [ 0 ] count_cont = s [ 0 ] # number of observations mean_cont = s [ 1 ] # mean std_cont = s [ 2 ] # standard deviation conf_level = s [ 4 ] # confidence level # this for loop goes over variants and compares one variant values against control variant values for # all metrics at once. Similar to scipy.stats.ttest_ind_from_stats for i in range ( variants_count ): # treatment variant data s = stats_values [ i ] count_treat = s [ 0 ] # number of observations mean_treat = s [ 1 ] # mean std_treat = s [ 2 ] # standard deviation # degrees of freedom num = ( std_cont ** 2 / count_cont + std_treat ** 2 / count_treat ) ** 2 den = ( std_cont ** 4 / ( count_cont ** 2 * ( count_cont - 1 ))) + ( std_treat ** 4 / ( count_treat ** 2 * ( count_treat - 1 )) ) with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): # We fill in zeros, when goal data are missing for some variant. # There could be division by zero here which is expected as we return # nan or inf values to the caller. f = num / den # degrees of freedom # t-quantile with warnings . catch_warnings (): warnings . filterwarnings ( 'ignore' , category = RuntimeWarning ) t_quantile = st . t . ppf ( conf_level + ( 1 - conf_level ) / 2 , f ) # right quantile # relative difference and test statistics with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): # We fill in zeros, when goal data are missing for some variant. # There could be division by zero here which is expected as we return # nan or inf values to the caller. rel_diff = ( mean_treat - mean_cont ) / mean_cont # standard error for relative difference rel_se = ( np . sqrt ( ( mean_treat * std_cont ) ** 2 / ( mean_cont ** 2 * count_cont ) + ( std_treat ** 2 / count_treat ) ) / mean_cont ) test_stat = rel_diff / rel_se # p-value with warnings . catch_warnings (): warnings . filterwarnings ( 'ignore' , category = RuntimeWarning ) pval = 2 * ( 1 - st . t . cdf ( np . abs ( test_stat ), f )) # confidence interval conf_int = rel_se * t_quantile # save results stat_res . append (( rel_diff , test_stat , pval , conf_int , rel_se , f )) # Tune up results s = np . hstack ([ stats , stat_res ]) s = s . transpose ( 2 , 0 , 1 ) # move back to metrics, variants, stats order x , y , z = s . shape arr = s . reshape ( x * y , z ) # Output dataframe col = [ 'metric_id' , 'metric_name' , 'exp_variant_id' , 'count' , 'mean' , 'std' , 'sum_value' , 'confidence_level' , 'diff' , 'test_stat' , 'p_value' , 'confidence_interval' , 'standard_error' , 'degrees_of_freedom' , ] r = pd . DataFrame ( arr , columns = col ) return r","title":"Statistics"},{"location":"api/statistics.html#statistics","text":"","title":"Statistics"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics","text":"Various methods needed to evaluate experiment.","title":"epstats.toolkit.statistics.Statistics"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.multiple_comparisons_correction","text":"Holm-Bonferroni correction for multiple comparisons problem. It is applied when we have more than two variants, i.e. we have one control variant and at least two treatment variants. It adjusts p-value and length of confidence interval - both to be more conservative. Complete manual Algorithm: For each metric, select (unadjusted) p-values and replace them with adjusted ones. Based on adjustment ratio, compute new (adjusted) confidence intervals and replace old (unadjusted) ones. Parameters: Name Type Description Default df DataFrame dataframe as output of [ ttest_evaluation ][epstats.toolkit.statistics.Statistics.ttest_evaluation] required variants int number of variants in the experiment required metrics int number of metrics of experiment required confidence_level float desired confidence level at the end of the experiment, e.g. 0.95 required Returns: Type Description DataFrame dataframe of the same format as input with adjusted p-values and confidence intervals. Source code in epstats/toolkit/statistics.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 @classmethod def multiple_comparisons_correction ( cls , df : pd . DataFrame , variants : int , metrics : int , confidence_level : float ) -> pd . DataFrame : \"\"\" [Holm-Bonferroni correction](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method) for multiple comparisons problem. It is applied when we have more than two variants, i.e. we have one control variant and at least two treatment variants. It adjusts p-value and length of confidence interval - both to be more conservative. [Complete manual](../stats/multiple.md) Algorithm: For each metric, select (unadjusted) p-values and replace them with adjusted ones. Based on adjustment ratio, compute new (adjusted) confidence intervals and replace old (unadjusted) ones. Arguments: df: dataframe as output of [`ttest_evaluation`][epstats.toolkit.statistics.Statistics.ttest_evaluation] variants: number of variants in the experiment metrics: number of metrics of experiment confidence_level: desired confidence level at the end of the experiment, e.g. 0.95 Returns: dataframe of the same format as input with adjusted p-values and confidence intervals. \"\"\" alpha = 1 - confidence_level # level of significance for m in range ( metrics ): # indices of rows with metric m data index_from = m * variants + 1 index_to = ( m + 1 ) * variants - 1 # p-value adjustment pvals = df . loc [ index_from : index_to , 'p_value' ] . to_list () # select old p-values adj_pvals = multipletests ( pvals = pvals , alpha = alpha , method = 'holm' )[ 1 ] # compute adjusted p-values # confidence interval adjustment # we set ratio to 1 when test_stat is so big that pvals are zero, no reason to update ci adj_ratio = np . nan_to_num ( pvals / adj_pvals , nan = 1 ) # adjustment ratio adj_alpha = adj_ratio * alpha # adjusted level alpha f = df . loc [ index_from : index_to , 'degrees_of_freedom' ] . to_list () # degrees of freedom se = df . loc [ index_from : index_to , 'standard_error' ] . to_list () # standard error t_quantile = st . t . ppf ( np . ones ( variants - 1 ) - adj_alpha + adj_alpha / 2 , f ) # right t-quantile adj_conf_int = se * t_quantile # adjusted confidence interval # replace (unadjusted) p-values and confidence intervals with new adjusted ones df . loc [ index_from : index_to , 'p_value' ] = adj_pvals df . loc [ index_from : index_to , 'confidence_interval' ] = adj_conf_int return df","title":"multiple_comparisons_correction()"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.obf_alpha_spending_function","text":"O'Brien-Fleming alpha spending function . We adjust confidence level in time in experiment. Confidence level in this setting is a decreasing function of experiment time. See Sequential Analysis for details. Parameters: Name Type Description Default confidence_level int required confidence level at the end of the test, e.g. 0.95 required total_length int length of the test in days, e.g. 7, 14, 21 required actual_day int actual days in the experiment period, must be between 1 and total_length required Returns: Type Description int adjusted confidence level with respect to actual day of the experiment and total length of the experiment. Source code in epstats/toolkit/statistics.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 @classmethod def obf_alpha_spending_function ( cls , confidence_level : int , total_length : int , actual_day : int ) -> int : \"\"\" [O'Brien-Fleming alpha spending function](https://online.stat.psu.edu/stat509/node/80/). We adjust confidence level in time in experiment. Confidence level in this setting is a decreasing function of experiment time. See [Sequential Analysis](../stats/sequential.md) for details. Arguments: confidence_level: required confidence level at the end of the test, e.g. 0.95 total_length: length of the test in days, e.g. 7, 14, 21 actual_day: actual days in the experiment period, must be between 1 and `total_length` Returns: adjusted confidence level with respect to actual day of the experiment and total length of the experiment. \"\"\" alpha = 1 - confidence_level t = actual_day / total_length # t in (0, 1] q = st . norm . ppf ( 1 - alpha / 2 ) # quantile of normal distribution alpha_adj = 2 - 2 * st . norm . cdf ( q / np . sqrt ( t )) return 1 - alpha_adj","title":"obf_alpha_spending_function()"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.ttest_evaluation","text":"Testing statistical significance of relative difference in means of treatment and control variant. This is inspired by scipy.stats.ttest_ind_from_stats method that returns many more statistics than p-value and test statistic. Statistics used: Welch's t-test Welch\u2013Satterthwaite equation approximation of degrees of freedom. Parameters: Name Type Description Default stats <built-in function array> array with dimensions (metrics, variants, stats) required stats array values: metric_id metric_name exp_variant_id count mean std sum_value sum_sqr_value Returns: Type Description DataFrame dataframe containing statistics from the t-test Schema of returned dataframe: metric_id - metric id as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition metric_name - metric name as in [ Experiment ][epstats.toolkit.experiment.Experiment] definition exp_variant_id - variant id count - number of exposures, value of metric denominator mean - sum_value / count std - sample standard deviation sum_value - value of goals, value of metric nominator confidence_level - current confidence level used to calculate p_value and confidence_interval diff - relative diff between sample means of this and control variant test_stat - value of test statistic of the relative difference in means p_value - p-value of the test statistic under current confidence_level confidence_interval - confidence interval of the diff under current confidence_level standard_error - standard error of the diff degrees_of_freedom - degrees of freedom of this variant mean Source code in epstats/toolkit/statistics.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @classmethod def ttest_evaluation ( cls , stats : np . array ) -> pd . DataFrame : \"\"\" Testing statistical significance of relative difference in means of treatment and control variant. This is inspired by [scipy.stats.ttest_ind_from_stats](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html) method that returns many more statistics than p-value and test statistic. Statistics used: 1. [Welch's t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test) 1. [Welch\u2013Satterthwaite equation](https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation) approximation of degrees of freedom. Arguments: stats: array with dimensions (metrics, variants, stats) `stats` array values: 0. `metric_id` 1. `metric_name` 1. `exp_variant_id` 1. `count` 1. `mean` 1. `std` 1. `sum_value` 1. `sum_sqr_value` Returns: dataframe containing statistics from the t-test Schema of returned dataframe: 1. `metric_id` - metric id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `metric_name` - metric name as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition 1. `exp_variant_id` - variant id 1. `count` - number of exposures, value of metric denominator 1. `mean` - `sum_value` / `count` 1. `std` - sample standard deviation 1. `sum_value` - value of goals, value of metric nominator 1. `confidence_level` - current confidence level used to calculate `p_value` and `confidence_interval` 1. `diff` - relative diff between sample means of this and control variant 1. `test_stat` - value of test statistic of the relative difference in means 1. `p_value` - p-value of the test statistic under current `confidence_level` 1. `confidence_interval` - confidence interval of the `diff` under current `confidence_level` 1. `standard_error` - standard error of the `diff` 1. `degrees_of_freedom` - degrees of freedom of this variant mean \"\"\" stats = stats . transpose ( 1 , 2 , 0 ) stat_res = [] # semiresults variants_count = stats . shape [ 0 ] # number of variants # get only stats (not metric_id, metric_name, exp_variant_id) from the stats array as floats stats_values = stats [:, 3 : 8 , :] . astype ( float ) # control variant values s = stats_values [ 0 ] count_cont = s [ 0 ] # number of observations mean_cont = s [ 1 ] # mean std_cont = s [ 2 ] # standard deviation conf_level = s [ 4 ] # confidence level # this for loop goes over variants and compares one variant values against control variant values for # all metrics at once. Similar to scipy.stats.ttest_ind_from_stats for i in range ( variants_count ): # treatment variant data s = stats_values [ i ] count_treat = s [ 0 ] # number of observations mean_treat = s [ 1 ] # mean std_treat = s [ 2 ] # standard deviation # degrees of freedom num = ( std_cont ** 2 / count_cont + std_treat ** 2 / count_treat ) ** 2 den = ( std_cont ** 4 / ( count_cont ** 2 * ( count_cont - 1 ))) + ( std_treat ** 4 / ( count_treat ** 2 * ( count_treat - 1 )) ) with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): # We fill in zeros, when goal data are missing for some variant. # There could be division by zero here which is expected as we return # nan or inf values to the caller. f = num / den # degrees of freedom # t-quantile with warnings . catch_warnings (): warnings . filterwarnings ( 'ignore' , category = RuntimeWarning ) t_quantile = st . t . ppf ( conf_level + ( 1 - conf_level ) / 2 , f ) # right quantile # relative difference and test statistics with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): # We fill in zeros, when goal data are missing for some variant. # There could be division by zero here which is expected as we return # nan or inf values to the caller. rel_diff = ( mean_treat - mean_cont ) / mean_cont # standard error for relative difference rel_se = ( np . sqrt ( ( mean_treat * std_cont ) ** 2 / ( mean_cont ** 2 * count_cont ) + ( std_treat ** 2 / count_treat ) ) / mean_cont ) test_stat = rel_diff / rel_se # p-value with warnings . catch_warnings (): warnings . filterwarnings ( 'ignore' , category = RuntimeWarning ) pval = 2 * ( 1 - st . t . cdf ( np . abs ( test_stat ), f )) # confidence interval conf_int = rel_se * t_quantile # save results stat_res . append (( rel_diff , test_stat , pval , conf_int , rel_se , f )) # Tune up results s = np . hstack ([ stats , stat_res ]) s = s . transpose ( 2 , 0 , 1 ) # move back to metrics, variants, stats order x , y , z = s . shape arr = s . reshape ( x * y , z ) # Output dataframe col = [ 'metric_id' , 'metric_name' , 'exp_variant_id' , 'count' , 'mean' , 'std' , 'sum_value' , 'confidence_level' , 'diff' , 'test_stat' , 'p_value' , 'confidence_interval' , 'standard_error' , 'degrees_of_freedom' , ] r = pd . DataFrame ( arr , columns = col ) return r","title":"ttest_evaluation()"},{"location":"api/test_data.html","text":"Test Data \u00b6 \u00b6 Utility methods to load sample (test) data that are used in unit tests through this project. load_evaluations_checks ( exp_id = None ) classmethod \u00b6 Load checks (SRM) evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 45 46 47 48 49 50 51 52 53 54 55 56 @classmethod def load_evaluations_checks ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load checks (SRM) evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'evaluations_checks.csv' ), sep = ' \\t ' ) return df [ df . exp_id == exp_id ] if exp_id is not None else df load_evaluations_exposures ( exp_id = None ) classmethod \u00b6 Load exposures evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 58 59 60 61 62 63 64 65 66 67 68 69 @classmethod def load_evaluations_exposures ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load exposures evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'evaluations_exposures.csv' ), sep = ' \\t ' ) return df [ df . exp_id == exp_id ] if exp_id is not None else df load_evaluations_metrics ( exp_id = None ) classmethod \u00b6 Load metric evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 71 72 73 74 75 76 77 78 79 80 81 82 @classmethod def load_evaluations_metrics ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load metric evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'evaluations_metrics.csv' ), sep = ' \\t ' ) return df [ df . exp_id == exp_id ] if exp_id is not None else df load_goals_agg ( exp_id = None ) classmethod \u00b6 Load sample of aggregated test data to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too. See load_evaluations set of functions to load corresponding evaluation results. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @classmethod def load_goals_agg ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load sample of aggregated test data to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too. See `load_evaluations` set of functions to load corresponding evaluation results. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'goals_agg.csv' ), sep = ' \\t ' ) . fillna ( { 'dimension' : '' , 'dimension_value' : '' } ) return df [ df . exp_id == exp_id ] if exp_id is not None else df load_goals_by_unit ( exp_id = None ) classmethod \u00b6 Load sample of test data by unit to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too. See load_evaluations set of functions to load corresponding evaluation results. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @classmethod def load_goals_by_unit ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load sample of test data by unit to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too. See `load_evaluations` set of functions to load corresponding evaluation results. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'goals_by_unit.csv' ), sep = ' \\t ' ) . fillna ( { 'dimension' : '' , 'dimension_value' : '' } ) return df [ df . exp_id == exp_id ] if exp_id is not None else df","title":"TestData"},{"location":"api/test_data.html#test-data","text":"","title":"Test Data"},{"location":"api/test_data.html#epstats.toolkit.testing.test_data.TestData","text":"Utility methods to load sample (test) data that are used in unit tests through this project.","title":"epstats.toolkit.testing.test_data.TestData"},{"location":"api/test_data.html#epstats.toolkit.testing.test_data.TestData.load_evaluations_checks","text":"Load checks (SRM) evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 45 46 47 48 49 50 51 52 53 54 55 56 @classmethod def load_evaluations_checks ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load checks (SRM) evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'evaluations_checks.csv' ), sep = ' \\t ' ) return df [ df . exp_id == exp_id ] if exp_id is not None else df","title":"load_evaluations_checks()"},{"location":"api/test_data.html#epstats.toolkit.testing.test_data.TestData.load_evaluations_exposures","text":"Load exposures evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 58 59 60 61 62 63 64 65 66 67 68 69 @classmethod def load_evaluations_exposures ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load exposures evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'evaluations_exposures.csv' ), sep = ' \\t ' ) return df [ df . exp_id == exp_id ] if exp_id is not None else df","title":"load_evaluations_exposures()"},{"location":"api/test_data.html#epstats.toolkit.testing.test_data.TestData.load_evaluations_metrics","text":"Load metric evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 71 72 73 74 75 76 77 78 79 80 81 82 @classmethod def load_evaluations_metrics ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load metric evaluations results. This data can be used to do asserts against after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg] or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'evaluations_metrics.csv' ), sep = ' \\t ' ) return df [ df . exp_id == exp_id ] if exp_id is not None else df","title":"load_evaluations_metrics()"},{"location":"api/test_data.html#epstats.toolkit.testing.test_data.TestData.load_goals_agg","text":"Load sample of aggregated test data to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too. See load_evaluations set of functions to load corresponding evaluation results. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @classmethod def load_goals_agg ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load sample of aggregated test data to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too. See `load_evaluations` set of functions to load corresponding evaluation results. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'goals_agg.csv' ), sep = ' \\t ' ) . fillna ( { 'dimension' : '' , 'dimension_value' : '' } ) return df [ df . exp_id == exp_id ] if exp_id is not None else df","title":"load_goals_agg()"},{"location":"api/test_data.html#epstats.toolkit.testing.test_data.TestData.load_goals_by_unit","text":"Load sample of test data by unit to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too. See load_evaluations set of functions to load corresponding evaluation results. Parameters: Name Type Description Default exp_id str experiment id None Source code in epstats/toolkit/testing/test_data.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @classmethod def load_goals_by_unit ( cls , exp_id : str = None ) -> pd . DataFrame : \"\"\" Load sample of test data by unit to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too. See `load_evaluations` set of functions to load corresponding evaluation results. Arguments: exp_id: experiment id \"\"\" df = pd . read_csv ( pkg_resources . open_text ( resources , 'goals_by_unit.csv' ), sep = ' \\t ' ) . fillna ( { 'dimension' : '' , 'dimension_value' : '' } ) return df [ df . exp_id == exp_id ] if exp_id is not None else df","title":"load_goals_by_unit()"},{"location":"stats/basics.html","text":"Statistics \u00b6 We want to test, whether the difference between treatment and control variant in chosen metric is statistically significant or not. From statistical point of view, we deal with point estimates, confidence intervals and hypothesis testing. Confidence level is set by default to 95%, i.e. we talk about 95%-confidence intervals and hypothesis testing on 5% level of significance. We also run various data quality checks to guarantee trustworthiness of presented data. Point Estimates, Confidence Intervals and Hypothesis Testing \u00b6 Assume we have one control variant (usually denoted by A A ) and one or more treatment variants (usually denoted by B B , C C , D D , ...). We calculate point estimate for relative difference between treatment and control variant in chosen metric. Next we calculate confidence interval for the relative difference. Finally, we want to test whether estimated relative difference is significantly significant. Formula for relative difference is straightforward: (B - A) / A * 100 (B - A) / A * 100 . To be more precise, point estimate is only the estimate of true relative difference between treatment and control variant. The true relative difference is unknown! The point estimate is the best possible estimate of the true (unknown) relative difference using available data from the experiment. We implemented Welch's t-test 1 to test the significance of point estimate in EP Stats engine. Delta method is necessary, since standard Welch's test works well only for absolute differences, i.e. B - A B - A . We use quantiles from Student's t-distribution which approximate normal distribution. Pitfalls and Corrections \u00b6 We summarize all corrections implemented next to Welch's t-test in this part. Generally Welch's t-test assumes we have two samples, i.e. we have two sequences of independent and identically distributed random variables with finite variance. The variances between the variants may not be equal. Unfortunately assumptions above are often violated in practice and appropriate corrections are necessary to guarantee desired level of significance. Absolute vs. Relative Difference \u00b6 Two-sample t-test is only correct when we deal with absolute difference, i.e. B - A B - A . But it does not hold any more when we deal with relative difference. In order to be statistically correct, delta method for relative difference is necessary. Independent and Identically Distributed Observations \u00b6 Welch's t-test assumes that observations are independent and identically distributed (i.i.d.). Unfortunately this assumption does not hold always. Let's assume Click-through rate metric (i.e. clicks / views). Since multiple views (and clicks) from the same user (user being randomization unit here) are allowed, the assumption of independence is violated. Multiple observations from the same user are not independent. Delta method for iid is necessary (has not been implemented yet). Multiple Comparisons Problem \u00b6 If we have only one control A A and one treatment B B variant, we need to run just one Welch's t-test, i.e. relative difference between B B and A A . If we have multiple treatment variants, e.g. B B , C C and D D , we need to run three Welch's t-tests, i.e. relative difference between B B and A A , C C and A A , D D and A A . If we run every single test on 5% level of significance, the overall level of significance is lower. The probability of false-positive error (i.e. we wrongly reject at least one null hypothesis) is higher than required 5% level. In ep-stats engine, we implemented HolmBonferroni p-value correction . Single tests are more conservative and so overall level of confidence is satisfied. Real-time Experiment Evaluation \u00b6 Time is Money One of the main goals in Experimentation Platform was to develop real-time data pipelines. Standard Welch's t-test assumes you evaluate experiment only once, after you collect all data. If you evaluate experiment more than once, than the false-positive error (wrongly rejecting null hypothesis) grows enormously 2 . We implemented Sequential testing procedure to tackle this issue. It allows us to evaluate experiments (hypothesis) real-time during the experiment period without exceeding false-positive errors. The solution itself is pretty simple. In the beginning (circa first half of the experiment period) the decision rule is very conservative and only great differences can be called statistically significant. As we approach the end of the experiment the decision rule is less and less stringent. If we have enough evidence, the difference can be statistically significant and we can end up the experiment earlier. Main disadvantage is that the length of the experiment must be set in advance - before starting the experiment. This is very annoying for the experimenters (test owners) but right now this is the only way, how to deal with this issue. If we do not use Sequential testing, our false-positive errors could be somewhere between 20-30%, instead of required below 5%. It means one third of all presented results are wrong and without any chance to fix them. In other words, one third of decisions are wrong. Data Quality Checks \u00b6 Experimentation Platform is very complex. From data collection to statistical evaluation, there are many intermediate steps. All of these steps must be checked regularly in order to guarantee trustworthiness of presented results. Sample Ratio Mismatch Check \u00b6 Sample Ratio Mismatch check 3 (SRM Check) checks the quality of randomization. Randomization is absolutely crucial. Wrong randomization can have fatal consequences and results might be highly misleading. SRM check checks whether we have the same number of users in each variant. Chi-square[^6] test is implemented. In this check we require high reliability. Therefore the confidence level is set to 99.9%. If SRM check fails, presented results should not be taken into account and any decisions based on this experiment should not be done! Data Quality \u00b6 We have not implemented any automatic data quality checks yet. The plan is to check all parts of data pipelines. Welch's t-test \u21a9 Ronny Kohavi, Sample Ratio Mismatch \u21a9 Wikipedia, Chi-square test \u21a9","title":"Basics"},{"location":"stats/basics.html#statistics","text":"We want to test, whether the difference between treatment and control variant in chosen metric is statistically significant or not. From statistical point of view, we deal with point estimates, confidence intervals and hypothesis testing. Confidence level is set by default to 95%, i.e. we talk about 95%-confidence intervals and hypothesis testing on 5% level of significance. We also run various data quality checks to guarantee trustworthiness of presented data.","title":"Statistics"},{"location":"stats/basics.html#point-estimates-confidence-intervals-and-hypothesis-testing","text":"Assume we have one control variant (usually denoted by A A ) and one or more treatment variants (usually denoted by B B , C C , D D , ...). We calculate point estimate for relative difference between treatment and control variant in chosen metric. Next we calculate confidence interval for the relative difference. Finally, we want to test whether estimated relative difference is significantly significant. Formula for relative difference is straightforward: (B - A) / A * 100 (B - A) / A * 100 . To be more precise, point estimate is only the estimate of true relative difference between treatment and control variant. The true relative difference is unknown! The point estimate is the best possible estimate of the true (unknown) relative difference using available data from the experiment. We implemented Welch's t-test 1 to test the significance of point estimate in EP Stats engine. Delta method is necessary, since standard Welch's test works well only for absolute differences, i.e. B - A B - A . We use quantiles from Student's t-distribution which approximate normal distribution.","title":"Point Estimates, Confidence Intervals and Hypothesis Testing"},{"location":"stats/basics.html#pitfalls-and-corrections","text":"We summarize all corrections implemented next to Welch's t-test in this part. Generally Welch's t-test assumes we have two samples, i.e. we have two sequences of independent and identically distributed random variables with finite variance. The variances between the variants may not be equal. Unfortunately assumptions above are often violated in practice and appropriate corrections are necessary to guarantee desired level of significance.","title":"Pitfalls and Corrections"},{"location":"stats/basics.html#absolute-vs-relative-difference","text":"Two-sample t-test is only correct when we deal with absolute difference, i.e. B - A B - A . But it does not hold any more when we deal with relative difference. In order to be statistically correct, delta method for relative difference is necessary.","title":"Absolute vs. Relative Difference"},{"location":"stats/basics.html#independent-and-identically-distributed-observations","text":"Welch's t-test assumes that observations are independent and identically distributed (i.i.d.). Unfortunately this assumption does not hold always. Let's assume Click-through rate metric (i.e. clicks / views). Since multiple views (and clicks) from the same user (user being randomization unit here) are allowed, the assumption of independence is violated. Multiple observations from the same user are not independent. Delta method for iid is necessary (has not been implemented yet).","title":"Independent and Identically Distributed Observations"},{"location":"stats/basics.html#multiple-comparisons-problem","text":"If we have only one control A A and one treatment B B variant, we need to run just one Welch's t-test, i.e. relative difference between B B and A A . If we have multiple treatment variants, e.g. B B , C C and D D , we need to run three Welch's t-tests, i.e. relative difference between B B and A A , C C and A A , D D and A A . If we run every single test on 5% level of significance, the overall level of significance is lower. The probability of false-positive error (i.e. we wrongly reject at least one null hypothesis) is higher than required 5% level. In ep-stats engine, we implemented HolmBonferroni p-value correction . Single tests are more conservative and so overall level of confidence is satisfied.","title":"Multiple Comparisons Problem"},{"location":"stats/basics.html#real-time-experiment-evaluation","text":"Time is Money One of the main goals in Experimentation Platform was to develop real-time data pipelines. Standard Welch's t-test assumes you evaluate experiment only once, after you collect all data. If you evaluate experiment more than once, than the false-positive error (wrongly rejecting null hypothesis) grows enormously 2 . We implemented Sequential testing procedure to tackle this issue. It allows us to evaluate experiments (hypothesis) real-time during the experiment period without exceeding false-positive errors. The solution itself is pretty simple. In the beginning (circa first half of the experiment period) the decision rule is very conservative and only great differences can be called statistically significant. As we approach the end of the experiment the decision rule is less and less stringent. If we have enough evidence, the difference can be statistically significant and we can end up the experiment earlier. Main disadvantage is that the length of the experiment must be set in advance - before starting the experiment. This is very annoying for the experimenters (test owners) but right now this is the only way, how to deal with this issue. If we do not use Sequential testing, our false-positive errors could be somewhere between 20-30%, instead of required below 5%. It means one third of all presented results are wrong and without any chance to fix them. In other words, one third of decisions are wrong.","title":"Real-time Experiment Evaluation"},{"location":"stats/basics.html#data-quality-checks","text":"Experimentation Platform is very complex. From data collection to statistical evaluation, there are many intermediate steps. All of these steps must be checked regularly in order to guarantee trustworthiness of presented results.","title":"Data Quality Checks"},{"location":"stats/basics.html#sample-ratio-mismatch-check","text":"Sample Ratio Mismatch check 3 (SRM Check) checks the quality of randomization. Randomization is absolutely crucial. Wrong randomization can have fatal consequences and results might be highly misleading. SRM check checks whether we have the same number of users in each variant. Chi-square[^6] test is implemented. In this check we require high reliability. Therefore the confidence level is set to 99.9%. If SRM check fails, presented results should not be taken into account and any decisions based on this experiment should not be done!","title":"Sample Ratio Mismatch Check"},{"location":"stats/basics.html#data-quality","text":"We have not implemented any automatic data quality checks yet. The plan is to check all parts of data pipelines. Welch's t-test \u21a9 Ronny Kohavi, Sample Ratio Mismatch \u21a9 Wikipedia, Chi-square test \u21a9","title":"Data Quality"},{"location":"stats/ctr.html","text":"CTR Metric \u00b6 The goal of this post is to provide complete manual for deriving asymptotic distribution of Click-through rate (CTR). We are aimed at correct theoretical derivations, including verification of all assumptions. Since CTR is one of the primary metrics in A/B testing, we derive asymptotic distribution for absolute and relative difference in CTR between two variants - control and treatment. Theory \u00b6 We will use two core statistical tools - Central limit theorem and Delta method . There exists a great YouTube video from the Khan Academy explaining CLT, we greatly recommend it! Central limit theorem \u00b6 Let X_{1}, \\dots, X_{n} X_{1}, \\dots, X_{n} be a random sample of size n n - a sequence of n n independent and identically distributed random variables drawn from a distribution of expected value given by \\mu \\mu and finite variance given by \\sigma^{2} \\sigma^{2} . Let denote sample average as \\bar{X_{n}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} \\bar{X_{n}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} . Then holds \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} (0, \\sigma^{2}), \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} (0, \\sigma^{2}), \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma^{2}), \\,\\,\\, n \\rightarrow \\, \\infty \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma^{2}), \\,\\,\\, n \\rightarrow \\, \\infty i.e. as n n approaches infinity, the random variables \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) converge in distribution to a normal \\mathcal{N} (0, \\sigma^{2}) \\mathcal{N} (0, \\sigma^{2}) . Another acceptable, but slightly vague formulations are \\bar{X_{n}} \\stackrel{as}{\\sim} \\mathcal{N} (\\mu, \\frac{\\sigma^{2}}{n}), \\bar{X_{n}} \\stackrel{as}{\\sim} \\mathcal{N} (\\mu, \\frac{\\sigma^{2}}{n}), or \\sqrt{n} \\, \\frac{\\bar{X_{n}} - \\mu}{\\sigma} \\stackrel{as}{\\sim} \\mathcal{N} (0, 1). \\sqrt{n} \\, \\frac{\\bar{X_{n}} - \\mu}{\\sigma} \\stackrel{as}{\\sim} \\mathcal{N} (0, 1). Delta Method - Univariate \u00b6 Let assume any sequence of random variables \\{T_{n}\\}_{n=1}^{\\infty} \\{T_{n}\\}_{n=1}^{\\infty} satisfying \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big) \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big) \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty and function g: \\mathbb{R} \\rightarrow \\mathbb{R} g: \\mathbb{R} \\rightarrow \\mathbb{R} which has continuous derivative around a point \\mu \\mu , i.e. g^{'}(\\mu) g^{'}(\\mu) is continuous. Then holds \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big). \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big). \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty. \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty. Delta Method - Multivariate \u00b6 Let assume any sequence of random vectors \\{ \\pmb{T}_{n} \\}_{n=1}^{\\infty} \\{ \\pmb{T}_{n} \\}_{n=1}^{\\infty} satisfying \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big) \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big) \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big), \\,\\,\\, n \\rightarrow \\, \\infty \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big), \\,\\,\\, n \\rightarrow \\, \\infty and function g: \\mathbb{R^{k}} \\rightarrow \\mathbb{R^{p}} g: \\mathbb{R^{k}} \\rightarrow \\mathbb{R^{p}} which is continuously differentiable around point \\pmb{\\mu} \\pmb{\\mu} . Denote \\mathbb{D}(x) = \\frac{\\partial \\, g(x)}{\\partial \\, x} \\mathbb{D}(x) = \\frac{\\partial \\, g(x)}{\\partial \\, x} . Then holds \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big). \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big). \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big), \\,\\,\\, n \\rightarrow \\, \\infty. \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big), \\,\\,\\, n \\rightarrow \\, \\infty. CTR Definition \u00b6 Without loss of generality, we can only focus on the control group with K K users. Every user can see test screen multiple times, denoted by N_{i}, \\, i = 1, \\dots, K N_{i}, \\, i = 1, \\dots, K . N_{i} \\in \\mathbb{N} N_{i} \\in \\mathbb{N} is a discrete random variable with unknown probability distribution and finite variance. Next user can click on the screen. This action is denoted by binomial random variable Y_{i, j}, \\, i = 1, \\dots, K, \\, j = 1, \\dots, N_{i} Y_{i, j}, \\, i = 1, \\dots, K, \\, j = 1, \\dots, N_{i} Y_{i, j}=\\begin{cases} 1, & \\text{if $i-th$ user clicks in his $j-th$ view},\\\\ 0, & \\text{otherwise}. \\end{cases} Y_{i, j}=\\begin{cases} 1, & \\text{if $i-th$ user clicks in his $j-th$ view},\\\\ 0, & \\text{otherwise}. \\end{cases} Click-through rate (CTR) is then defined as sum of all clicks devided by sum of all views CTR = \\frac{\\sum_{i=1}^{K} \\sum_{j=1}^{N_{i}} Y_{i, j}}{\\sum_{i=1}^{K} N_{i}}. CTR = \\frac{\\sum_{i=1}^{K} \\sum_{j=1}^{N_{i}} Y_{i, j}}{\\sum_{i=1}^{K} N_{i}}. We want to derive asymptotic distribution for CTR. But we can not directly use central limit theorem since assumptions are violated. Random variables Y_{i, j} Y_{i, j} are not independent, nor identically distributed. We can use a little trick 1 and simply reformulate CTR definition without any change: CTR = \\frac{\\sum_{i=1}^{K} \\big( \\sum_{j=1}^{N_{i}} Y_{i, j} \\big)}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i}}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i} \\big/ K}{\\sum_{i=1}^{K} N_{i} \\big/ K} = \\frac{\\bar{S}}{\\bar{N}} = \\bar{Y}, CTR = \\frac{\\sum_{i=1}^{K} \\big( \\sum_{j=1}^{N_{i}} Y_{i, j} \\big)}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i}}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i} \\big/ K}{\\sum_{i=1}^{K} N_{i} \\big/ K} = \\frac{\\bar{S}}{\\bar{N}} = \\bar{Y}, where \\bar{S} = \\frac{1}{K} \\sum_{i=1}^{K} S_{i} \\bar{S} = \\frac{1}{K} \\sum_{i=1}^{K} S_{i} stands for average clicks per user and \\bar{N} = \\frac{1}{K} \\sum_{i=1}^{K} N_{i} \\bar{N} = \\frac{1}{K} \\sum_{i=1}^{K} N_{i} stands for average views per user. Users are independent of each other, random variables N_{i}, \\, i = 1, \\dots, K N_{i}, \\, i = 1, \\dots, K are independent and indetically distributed. For simplification we will assume that also random variables S_{i}, \\, i = 1, \\dots, K S_{i}, \\, i = 1, \\dots, K are independent and identically distributed, but it is only half true. S_{i} S_{i} are independent, since users are independent of each other, but they are not identically distributed - S_{1} S_{1} has some unknown discrete distribution on closed interval [0, N_{1}] [0, N_{1}] , S_{2} S_{2} has some unknown discrete distribution on closed interval [0, N_{2}] [0, N_{2}] and so on. Since N_{i} \\in \\mathbb{N}, \\, i = 1, \\dots, K N_{i} \\in \\mathbb{N}, \\, i = 1, \\dots, K are random variables and so P \\big(N_{i} = N_{j} \\big) \\neq 1 P \\big(N_{i} = N_{j} \\big) \\neq 1 for i \\neq j i \\neq j . There exist other versions of central limit theorem which only assume independence, e.g. Lyapunov CLT . Asymptotic Distribution of CTR \u00b6 In this part we will derive asymptotic distributon for CTR. CTR is defined as fraction of two random variables - \\bar{S} \\bar{S} and \\bar{N} \\bar{N} . We will proceed in three steps: We will use CLT and derive asymptotic distributions for both \\bar{S} \\bar{S} and \\bar{N} \\bar{N} . We will use delta method - multivariate and derive asymptotic distribution for CTR. Step 1 \u00b6 Since S_{1}, \\dots, S_{K} S_{1}, \\dots, S_{K} is a random sample, from CLT we have \\sqrt{K} \\, \\big( \\bar{S} - \\mu_{S} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{S}^{2}). \\sqrt{K} \\, \\big( \\bar{S} - \\mu_{S} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{S}^{2}). Since N_{1}, \\dots, N_{K} N_{1}, \\dots, N_{K} is a random sample, from CLT we similary have \\sqrt{K} \\, \\big( \\bar{N} - \\mu_{N} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{N}^{2}). \\sqrt{K} \\, \\big( \\bar{N} - \\mu_{N} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{N}^{2}). We can join both asymptotic normal distributions into two dimensional normal distribution \\sqrt{K} \\, \\Bigg( \\begin{pmatrix} \\bar{S} \\\\ \\bar{N} \\end{pmatrix} - \\begin{pmatrix} \\mu_{S} \\\\ \\mu_{N} \\end{pmatrix} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{S}^{2} & \\sigma_{SN} \\\\ \\sigma_{SN} & \\sigma_{N}^2 \\end{pmatrix} \\Bigg), \\sqrt{K} \\, \\Bigg( \\begin{pmatrix} \\bar{S} \\\\ \\bar{N} \\end{pmatrix} - \\begin{pmatrix} \\mu_{S} \\\\ \\mu_{N} \\end{pmatrix} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{S}^{2} & \\sigma_{SN} \\\\ \\sigma_{SN} & \\sigma_{N}^2 \\end{pmatrix} \\Bigg), where \\sigma_{SN} \\sigma_{SN} is covariance between random variables S S and N N defined as \\sigma_{SN} = \\mathrm{cov}(S,N) = \\mathbb{E} \\big[(S - \\mu_{S})(N - \\mu_{n})\\big] \\sigma_{SN} = \\mathrm{cov}(S,N) = \\mathbb{E} \\big[(S - \\mu_{S})(N - \\mu_{n})\\big] . Unknown covariance \\sigma_{SN} \\sigma_{SN} can be easily estimated using following formula \\hat{\\sigma_{SN}} = \\sum_{i=1}^{K} (S_{i} - \\bar{S}_{n}) (N_{i} - \\bar{N}_{n}) = \\sum_{i=1}^{K} S_{i} N_{i} - K \\bar{S}_{n} \\bar{N}_{n}. \\hat{\\sigma_{SN}} = \\sum_{i=1}^{K} (S_{i} - \\bar{S}_{n}) (N_{i} - \\bar{N}_{n}) = \\sum_{i=1}^{K} S_{i} N_{i} - K \\bar{S}_{n} \\bar{N}_{n}. Step 2 \u00b6 Now we apply multivariate delta method with a link function g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} defined as g(x, y) = \\frac{x}{y} g(x, y) = \\frac{x}{y} . Gradient in point (\\mu_{S}, \\mu_{N}) (\\mu_{S}, \\mu_{N}) equals to \\nabla g (\\mu_{S}, \\mu_{N}) = (\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}}). \\nabla g (\\mu_{S}, \\mu_{N}) = (\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}}). Hence we have \\sqrt{K} \\, \\Bigg( \\frac{\\bar{S}}{\\bar{N}} - \\frac{\\mu_{S}}{\\mu_{N}} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\Bigg(0, \\, \\big(\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}} \\big) \\begin{pmatrix} \\sigma_{S}^{2} & \\sigma_{SN} \\\\ \\sigma_{SN} & \\sigma_{N}^2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\mu_{N}} \\\\ -\\frac{\\mu_{S}}{\\mu_{N}} \\end{pmatrix} \\Bigg) \\sqrt{K} \\, \\Bigg( \\frac{\\bar{S}}{\\bar{N}} - \\frac{\\mu_{S}}{\\mu_{N}} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\Bigg(0, \\, \\big(\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}} \\big) \\begin{pmatrix} \\sigma_{S}^{2} & \\sigma_{SN} \\\\ \\sigma_{SN} & \\sigma_{N}^2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\mu_{N}} \\\\ -\\frac{\\mu_{S}}{\\mu_{N}} \\end{pmatrix} \\Bigg) Asymptotic distribution for CTR in treatment group with K K observations equals to $$ \\sqrt{K} \\, \\bigg( \\bar{Y} - \\mu_{Y} \\bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\bigg(0, \\, \\frac{1}{\\mu_{N}^2} \\big(\\sigma_{S}^2 - 2\\frac{\\mu_{S}}{\\mu_{N}}\\sigma_{SN} + \\frac{\\mu_{S}^2}{\\mu_{N}^2} \\sigma_{N}^2 \\big) \\bigg),$$ as K K approaches infinity. Difference Between Control and Treatment Group \u00b6 We have derived asymptotic distribution fot control group. Analogously we would have derived asymptotic distribution for treatment group. Let's write them both once again \\sqrt{K} \\, \\big( \\bar{Y}_{A} - \\mu_{A} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{A}^2 \\big),$$ $$\\sqrt{L} \\, \\big( \\bar{Y}_{B} - \\mu_{B} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{B}^2 \\big), \\sqrt{K} \\, \\big( \\bar{Y}_{A} - \\mu_{A} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{A}^2 \\big),$$ $$\\sqrt{L} \\, \\big( \\bar{Y}_{B} - \\mu_{B} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{B}^2 \\big), where \\sigma_{A}^2 \\sigma_{A}^2 and \\sigma_{B}^2 \\sigma_{B}^2 follows derivations right above (the complicated fomula) and K K and L L are number of observations in control and treatment group respectively. Since we have again two asymptotic normal distributions, we can join them into two dimensional normal distribution \\Bigg( \\begin{pmatrix} \\bar{Y}_{A} \\\\ \\bar{Y}_{B} \\end{pmatrix} - \\begin{pmatrix} \\mu_{A} \\\\ \\mu_{B} \\end{pmatrix} \\Bigg) \\stackrel{as}{\\sim} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{A}^{2} \\, / \\, K & 0 \\\\ 0 & \\sigma_{B}^2 \\, / \\, {L} \\end{pmatrix} \\Bigg). \\Bigg( \\begin{pmatrix} \\bar{Y}_{A} \\\\ \\bar{Y}_{B} \\end{pmatrix} - \\begin{pmatrix} \\mu_{A} \\\\ \\mu_{B} \\end{pmatrix} \\Bigg) \\stackrel{as}{\\sim} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{A}^{2} \\, / \\, K & 0 \\\\ 0 & \\sigma_{B}^2 \\, / \\, {L} \\end{pmatrix} \\Bigg). This time we used slightly different notation. We do need to be careful now. In general, we have different sample size ( K \\neq L K \\neq L ). But on the other hand, in this case we assume there is no correlation between those two distributions, see zeros in covariance matrix. In A/B testing we are usually interested in whether the difference between treatment and control group is statistically significant. We derive asymptotic distribution for both absolute and relative difference. Absolute Difference \u00b6 Absolute difference is easier. We will use multivariate delta method with simple link function g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} defined as g(x, y) = y - x g(x, y) = y - x . Be aware of order x x and y y - it is y - x y - x , not x - y x - y . Gradient in point (\\mu_{A}, \\mu_{B}) (\\mu_{A}, \\mu_{B}) equals to \\nabla g (\\mu_{A}, \\mu_{B}) = (-1, 1) \\nabla g (\\mu_{A}, \\mu_{B}) = (-1, 1) and hence the result is \\big( (\\bar{Y}_{B} - \\bar{Y}_{A}) - (\\mu_{B} - \\mu_{A}) \\big) \\stackrel{as}{\\sim} \\mathcal{N} \\big(0, \\, (\\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L}) \\big). \\big( (\\bar{Y}_{B} - \\bar{Y}_{A}) - (\\mu_{B} - \\mu_{A}) \\big) \\stackrel{as}{\\sim} \\mathcal{N} \\big(0, \\, (\\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L}) \\big). It can be written in following form Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), if K, L \\rightarrow \\infty K, L \\rightarrow \\infty and \\frac{K}{L} \\rightarrow q \\in (0, \\infty) \\frac{K}{L} \\rightarrow q \\in (0, \\infty) . S_{A}^2 S_{A}^2 and S_{B}^2 S_{B}^2 are sample variances. Two sided asymptotic confidence interval for absolute difference equals to \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big), \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big), where u_{1 - \\alpha / 2} u_{1 - \\alpha / 2} is (1 - \\alpha / 2)- (1 - \\alpha / 2)- quantile of normal distribution \\mathcal{N}(0, 1) \\mathcal{N}(0, 1) . P-value equals to $$ p = 2 \\big(1 - \\Phi(|z|) \\big), $$ where \\Phi \\Phi is distribution function of \\mathcal{N}(0, 1) \\mathcal{N}(0, 1) and z z is observed value of test statistics Z_{K,L} Z_{K,L} . In practice is usually used Welch test , which uses t-distribution instead of normal distribution, with f f degrees of freedom given as f = \\frac{\\big( \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} \\big)^2}{\\frac{S_{A}^4}{K^2 (K-1)} + \\frac{S_{B}^4}{L^2 (L-1)}} . f = \\frac{\\big( \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} \\big)^2}{\\frac{S_{A}^4}{K^2 (K-1)} + \\frac{S_{B}^4}{L^2 (L-1)}} . Then two sided asymptotic confidence interval (with t-quantiles) for absolute difference equals to \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big). \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big). P-value equals to $$ p = 2 \\big(1 - \\text{CDF}_{t, f}(|z|) \\big), $$ where \\text{CDF}_{t,f} \\text{CDF}_{t,f} is cumulative distribution function of t-distribution with f f degrees of freedom and z z is observed value of test statistics Z_{K,L} Z_{K,L} . Relative Difference \u00b6 To derive asymptotic distribution for relative difference we will again use multivariate delta method with a link function g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} defined as g(x, y) = \\frac{y - x}{x} g(x, y) = \\frac{y - x}{x} . Be aware of order x x and y y . Gradient in point (\\mu_{A}, \\mu_{B}) (\\mu_{A}, \\mu_{B}) equals to \\nabla g (\\mu_{A}, \\mu_{B}) = \\big( -\\frac{\\mu_{B}}{\\mu_{A}^2}, \\frac{1}{\\mu_{A}} \\big). \\nabla g (\\mu_{A}, \\mu_{B}) = \\big( -\\frac{\\mu_{B}}{\\mu_{A}^2}, \\frac{1}{\\mu_{A}} \\big). The result is \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} \\Big) \\stackrel{as}{\\sim} \\mathcal{N} \\Big(0, \\, \\frac{1}{\\mu_{A}^2} \\big(\\frac{\\mu_{B}^2}{\\mu_{A}^2} \\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L} \\big)\\Big). \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} \\Big) \\stackrel{as}{\\sim} \\mathcal{N} \\Big(0, \\, \\frac{1}{\\mu_{A}^2} \\big(\\frac{\\mu_{B}^2}{\\mu_{A}^2} \\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L} \\big)\\Big). This can be rewritten in following form Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), if K, L \\rightarrow \\infty K, L \\rightarrow \\infty and \\frac{K}{L} \\rightarrow q \\in (0, \\infty) \\frac{K}{L} \\rightarrow q \\in (0, \\infty) . S_{A}^2 S_{A}^2 and S_{B}^2 S_{B}^2 are sample variances. For unknown true relative difference \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} we can derive confidence interval. For simplicity let's denote the sample variance as \\tilde{S}^2 \\tilde{S}^2 , i.e.: \\tilde{S}^2 = \\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }. \\tilde{S}^2 = \\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }. Finaly, the two sided asymptotic confidence interval for relative difference equals to \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 ; \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} + \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 \\Big). \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 ; \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} + \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 \\Big). P-value equals to $$ p = 2 \\big(1 - \\Phi(|z|) \\big), $$ where \\Phi \\Phi is distribution function of \\mathcal{N}(0, 1) \\mathcal{N}(0, 1) and z z is observed value of test statistics Z_{K,L} Z_{K,L} . Since we know, there is no straightforward approximation using t-quantiles, because there is no formula for degrees of freedom. In practise, we have huge amount of observations and both quantiles (normal and t-quantile) are very close to each other for large n n . A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas \u21a9","title":"CTR Metric Redefined"},{"location":"stats/ctr.html#ctr-metric","text":"The goal of this post is to provide complete manual for deriving asymptotic distribution of Click-through rate (CTR). We are aimed at correct theoretical derivations, including verification of all assumptions. Since CTR is one of the primary metrics in A/B testing, we derive asymptotic distribution for absolute and relative difference in CTR between two variants - control and treatment.","title":"CTR Metric"},{"location":"stats/ctr.html#theory","text":"We will use two core statistical tools - Central limit theorem and Delta method . There exists a great YouTube video from the Khan Academy explaining CLT, we greatly recommend it!","title":"Theory"},{"location":"stats/ctr.html#central-limit-theorem","text":"Let X_{1}, \\dots, X_{n} X_{1}, \\dots, X_{n} be a random sample of size n n - a sequence of n n independent and identically distributed random variables drawn from a distribution of expected value given by \\mu \\mu and finite variance given by \\sigma^{2} \\sigma^{2} . Let denote sample average as \\bar{X_{n}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} \\bar{X_{n}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} . Then holds \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} (0, \\sigma^{2}), \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} (0, \\sigma^{2}), \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma^{2}), \\,\\,\\, n \\rightarrow \\, \\infty \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma^{2}), \\,\\,\\, n \\rightarrow \\, \\infty i.e. as n n approaches infinity, the random variables \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) converge in distribution to a normal \\mathcal{N} (0, \\sigma^{2}) \\mathcal{N} (0, \\sigma^{2}) . Another acceptable, but slightly vague formulations are \\bar{X_{n}} \\stackrel{as}{\\sim} \\mathcal{N} (\\mu, \\frac{\\sigma^{2}}{n}), \\bar{X_{n}} \\stackrel{as}{\\sim} \\mathcal{N} (\\mu, \\frac{\\sigma^{2}}{n}), or \\sqrt{n} \\, \\frac{\\bar{X_{n}} - \\mu}{\\sigma} \\stackrel{as}{\\sim} \\mathcal{N} (0, 1). \\sqrt{n} \\, \\frac{\\bar{X_{n}} - \\mu}{\\sigma} \\stackrel{as}{\\sim} \\mathcal{N} (0, 1).","title":"Central limit theorem"},{"location":"stats/ctr.html#delta-method-univariate","text":"Let assume any sequence of random variables \\{T_{n}\\}_{n=1}^{\\infty} \\{T_{n}\\}_{n=1}^{\\infty} satisfying \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big) \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big) \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty and function g: \\mathbb{R} \\rightarrow \\mathbb{R} g: \\mathbb{R} \\rightarrow \\mathbb{R} which has continuous derivative around a point \\mu \\mu , i.e. g^{'}(\\mu) g^{'}(\\mu) is continuous. Then holds \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big). \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big). \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty. \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty.","title":"Delta Method - Univariate"},{"location":"stats/ctr.html#delta-method-multivariate","text":"Let assume any sequence of random vectors \\{ \\pmb{T}_{n} \\}_{n=1}^{\\infty} \\{ \\pmb{T}_{n} \\}_{n=1}^{\\infty} satisfying \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big) \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big) \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big), \\,\\,\\, n \\rightarrow \\, \\infty \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big), \\,\\,\\, n \\rightarrow \\, \\infty and function g: \\mathbb{R^{k}} \\rightarrow \\mathbb{R^{p}} g: \\mathbb{R^{k}} \\rightarrow \\mathbb{R^{p}} which is continuously differentiable around point \\pmb{\\mu} \\pmb{\\mu} . Denote \\mathbb{D}(x) = \\frac{\\partial \\, g(x)}{\\partial \\, x} \\mathbb{D}(x) = \\frac{\\partial \\, g(x)}{\\partial \\, x} . Then holds \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big). \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big). \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big), \\,\\,\\, n \\rightarrow \\, \\infty. \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big), \\,\\,\\, n \\rightarrow \\, \\infty.","title":"Delta Method - Multivariate"},{"location":"stats/ctr.html#ctr-definition","text":"Without loss of generality, we can only focus on the control group with K K users. Every user can see test screen multiple times, denoted by N_{i}, \\, i = 1, \\dots, K N_{i}, \\, i = 1, \\dots, K . N_{i} \\in \\mathbb{N} N_{i} \\in \\mathbb{N} is a discrete random variable with unknown probability distribution and finite variance. Next user can click on the screen. This action is denoted by binomial random variable Y_{i, j}, \\, i = 1, \\dots, K, \\, j = 1, \\dots, N_{i} Y_{i, j}, \\, i = 1, \\dots, K, \\, j = 1, \\dots, N_{i} Y_{i, j}=\\begin{cases} 1, & \\text{if $i-th$ user clicks in his $j-th$ view},\\\\ 0, & \\text{otherwise}. \\end{cases} Y_{i, j}=\\begin{cases} 1, & \\text{if $i-th$ user clicks in his $j-th$ view},\\\\ 0, & \\text{otherwise}. \\end{cases} Click-through rate (CTR) is then defined as sum of all clicks devided by sum of all views CTR = \\frac{\\sum_{i=1}^{K} \\sum_{j=1}^{N_{i}} Y_{i, j}}{\\sum_{i=1}^{K} N_{i}}. CTR = \\frac{\\sum_{i=1}^{K} \\sum_{j=1}^{N_{i}} Y_{i, j}}{\\sum_{i=1}^{K} N_{i}}. We want to derive asymptotic distribution for CTR. But we can not directly use central limit theorem since assumptions are violated. Random variables Y_{i, j} Y_{i, j} are not independent, nor identically distributed. We can use a little trick 1 and simply reformulate CTR definition without any change: CTR = \\frac{\\sum_{i=1}^{K} \\big( \\sum_{j=1}^{N_{i}} Y_{i, j} \\big)}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i}}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i} \\big/ K}{\\sum_{i=1}^{K} N_{i} \\big/ K} = \\frac{\\bar{S}}{\\bar{N}} = \\bar{Y}, CTR = \\frac{\\sum_{i=1}^{K} \\big( \\sum_{j=1}^{N_{i}} Y_{i, j} \\big)}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i}}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i} \\big/ K}{\\sum_{i=1}^{K} N_{i} \\big/ K} = \\frac{\\bar{S}}{\\bar{N}} = \\bar{Y}, where \\bar{S} = \\frac{1}{K} \\sum_{i=1}^{K} S_{i} \\bar{S} = \\frac{1}{K} \\sum_{i=1}^{K} S_{i} stands for average clicks per user and \\bar{N} = \\frac{1}{K} \\sum_{i=1}^{K} N_{i} \\bar{N} = \\frac{1}{K} \\sum_{i=1}^{K} N_{i} stands for average views per user. Users are independent of each other, random variables N_{i}, \\, i = 1, \\dots, K N_{i}, \\, i = 1, \\dots, K are independent and indetically distributed. For simplification we will assume that also random variables S_{i}, \\, i = 1, \\dots, K S_{i}, \\, i = 1, \\dots, K are independent and identically distributed, but it is only half true. S_{i} S_{i} are independent, since users are independent of each other, but they are not identically distributed - S_{1} S_{1} has some unknown discrete distribution on closed interval [0, N_{1}] [0, N_{1}] , S_{2} S_{2} has some unknown discrete distribution on closed interval [0, N_{2}] [0, N_{2}] and so on. Since N_{i} \\in \\mathbb{N}, \\, i = 1, \\dots, K N_{i} \\in \\mathbb{N}, \\, i = 1, \\dots, K are random variables and so P \\big(N_{i} = N_{j} \\big) \\neq 1 P \\big(N_{i} = N_{j} \\big) \\neq 1 for i \\neq j i \\neq j . There exist other versions of central limit theorem which only assume independence, e.g. Lyapunov CLT .","title":"CTR Definition"},{"location":"stats/ctr.html#asymptotic-distribution-of-ctr","text":"In this part we will derive asymptotic distributon for CTR. CTR is defined as fraction of two random variables - \\bar{S} \\bar{S} and \\bar{N} \\bar{N} . We will proceed in three steps: We will use CLT and derive asymptotic distributions for both \\bar{S} \\bar{S} and \\bar{N} \\bar{N} . We will use delta method - multivariate and derive asymptotic distribution for CTR.","title":"Asymptotic Distribution of CTR"},{"location":"stats/ctr.html#step-1","text":"Since S_{1}, \\dots, S_{K} S_{1}, \\dots, S_{K} is a random sample, from CLT we have \\sqrt{K} \\, \\big( \\bar{S} - \\mu_{S} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{S}^{2}). \\sqrt{K} \\, \\big( \\bar{S} - \\mu_{S} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{S}^{2}). Since N_{1}, \\dots, N_{K} N_{1}, \\dots, N_{K} is a random sample, from CLT we similary have \\sqrt{K} \\, \\big( \\bar{N} - \\mu_{N} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{N}^{2}). \\sqrt{K} \\, \\big( \\bar{N} - \\mu_{N} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{N}^{2}). We can join both asymptotic normal distributions into two dimensional normal distribution \\sqrt{K} \\, \\Bigg( \\begin{pmatrix} \\bar{S} \\\\ \\bar{N} \\end{pmatrix} - \\begin{pmatrix} \\mu_{S} \\\\ \\mu_{N} \\end{pmatrix} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{S}^{2} & \\sigma_{SN} \\\\ \\sigma_{SN} & \\sigma_{N}^2 \\end{pmatrix} \\Bigg), \\sqrt{K} \\, \\Bigg( \\begin{pmatrix} \\bar{S} \\\\ \\bar{N} \\end{pmatrix} - \\begin{pmatrix} \\mu_{S} \\\\ \\mu_{N} \\end{pmatrix} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{S}^{2} & \\sigma_{SN} \\\\ \\sigma_{SN} & \\sigma_{N}^2 \\end{pmatrix} \\Bigg), where \\sigma_{SN} \\sigma_{SN} is covariance between random variables S S and N N defined as \\sigma_{SN} = \\mathrm{cov}(S,N) = \\mathbb{E} \\big[(S - \\mu_{S})(N - \\mu_{n})\\big] \\sigma_{SN} = \\mathrm{cov}(S,N) = \\mathbb{E} \\big[(S - \\mu_{S})(N - \\mu_{n})\\big] . Unknown covariance \\sigma_{SN} \\sigma_{SN} can be easily estimated using following formula \\hat{\\sigma_{SN}} = \\sum_{i=1}^{K} (S_{i} - \\bar{S}_{n}) (N_{i} - \\bar{N}_{n}) = \\sum_{i=1}^{K} S_{i} N_{i} - K \\bar{S}_{n} \\bar{N}_{n}. \\hat{\\sigma_{SN}} = \\sum_{i=1}^{K} (S_{i} - \\bar{S}_{n}) (N_{i} - \\bar{N}_{n}) = \\sum_{i=1}^{K} S_{i} N_{i} - K \\bar{S}_{n} \\bar{N}_{n}.","title":"Step 1"},{"location":"stats/ctr.html#step-2","text":"Now we apply multivariate delta method with a link function g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} defined as g(x, y) = \\frac{x}{y} g(x, y) = \\frac{x}{y} . Gradient in point (\\mu_{S}, \\mu_{N}) (\\mu_{S}, \\mu_{N}) equals to \\nabla g (\\mu_{S}, \\mu_{N}) = (\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}}). \\nabla g (\\mu_{S}, \\mu_{N}) = (\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}}). Hence we have \\sqrt{K} \\, \\Bigg( \\frac{\\bar{S}}{\\bar{N}} - \\frac{\\mu_{S}}{\\mu_{N}} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\Bigg(0, \\, \\big(\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}} \\big) \\begin{pmatrix} \\sigma_{S}^{2} & \\sigma_{SN} \\\\ \\sigma_{SN} & \\sigma_{N}^2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\mu_{N}} \\\\ -\\frac{\\mu_{S}}{\\mu_{N}} \\end{pmatrix} \\Bigg) \\sqrt{K} \\, \\Bigg( \\frac{\\bar{S}}{\\bar{N}} - \\frac{\\mu_{S}}{\\mu_{N}} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\Bigg(0, \\, \\big(\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}} \\big) \\begin{pmatrix} \\sigma_{S}^{2} & \\sigma_{SN} \\\\ \\sigma_{SN} & \\sigma_{N}^2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\mu_{N}} \\\\ -\\frac{\\mu_{S}}{\\mu_{N}} \\end{pmatrix} \\Bigg) Asymptotic distribution for CTR in treatment group with K K observations equals to $$ \\sqrt{K} \\, \\bigg( \\bar{Y} - \\mu_{Y} \\bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\bigg(0, \\, \\frac{1}{\\mu_{N}^2} \\big(\\sigma_{S}^2 - 2\\frac{\\mu_{S}}{\\mu_{N}}\\sigma_{SN} + \\frac{\\mu_{S}^2}{\\mu_{N}^2} \\sigma_{N}^2 \\big) \\bigg),$$ as K K approaches infinity.","title":"Step 2"},{"location":"stats/ctr.html#difference-between-control-and-treatment-group","text":"We have derived asymptotic distribution fot control group. Analogously we would have derived asymptotic distribution for treatment group. Let's write them both once again \\sqrt{K} \\, \\big( \\bar{Y}_{A} - \\mu_{A} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{A}^2 \\big),$$ $$\\sqrt{L} \\, \\big( \\bar{Y}_{B} - \\mu_{B} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{B}^2 \\big), \\sqrt{K} \\, \\big( \\bar{Y}_{A} - \\mu_{A} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{A}^2 \\big),$$ $$\\sqrt{L} \\, \\big( \\bar{Y}_{B} - \\mu_{B} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{B}^2 \\big), where \\sigma_{A}^2 \\sigma_{A}^2 and \\sigma_{B}^2 \\sigma_{B}^2 follows derivations right above (the complicated fomula) and K K and L L are number of observations in control and treatment group respectively. Since we have again two asymptotic normal distributions, we can join them into two dimensional normal distribution \\Bigg( \\begin{pmatrix} \\bar{Y}_{A} \\\\ \\bar{Y}_{B} \\end{pmatrix} - \\begin{pmatrix} \\mu_{A} \\\\ \\mu_{B} \\end{pmatrix} \\Bigg) \\stackrel{as}{\\sim} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{A}^{2} \\, / \\, K & 0 \\\\ 0 & \\sigma_{B}^2 \\, / \\, {L} \\end{pmatrix} \\Bigg). \\Bigg( \\begin{pmatrix} \\bar{Y}_{A} \\\\ \\bar{Y}_{B} \\end{pmatrix} - \\begin{pmatrix} \\mu_{A} \\\\ \\mu_{B} \\end{pmatrix} \\Bigg) \\stackrel{as}{\\sim} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{A}^{2} \\, / \\, K & 0 \\\\ 0 & \\sigma_{B}^2 \\, / \\, {L} \\end{pmatrix} \\Bigg). This time we used slightly different notation. We do need to be careful now. In general, we have different sample size ( K \\neq L K \\neq L ). But on the other hand, in this case we assume there is no correlation between those two distributions, see zeros in covariance matrix. In A/B testing we are usually interested in whether the difference between treatment and control group is statistically significant. We derive asymptotic distribution for both absolute and relative difference.","title":"Difference Between Control and Treatment Group"},{"location":"stats/ctr.html#absolute-difference","text":"Absolute difference is easier. We will use multivariate delta method with simple link function g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} defined as g(x, y) = y - x g(x, y) = y - x . Be aware of order x x and y y - it is y - x y - x , not x - y x - y . Gradient in point (\\mu_{A}, \\mu_{B}) (\\mu_{A}, \\mu_{B}) equals to \\nabla g (\\mu_{A}, \\mu_{B}) = (-1, 1) \\nabla g (\\mu_{A}, \\mu_{B}) = (-1, 1) and hence the result is \\big( (\\bar{Y}_{B} - \\bar{Y}_{A}) - (\\mu_{B} - \\mu_{A}) \\big) \\stackrel{as}{\\sim} \\mathcal{N} \\big(0, \\, (\\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L}) \\big). \\big( (\\bar{Y}_{B} - \\bar{Y}_{A}) - (\\mu_{B} - \\mu_{A}) \\big) \\stackrel{as}{\\sim} \\mathcal{N} \\big(0, \\, (\\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L}) \\big). It can be written in following form Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), if K, L \\rightarrow \\infty K, L \\rightarrow \\infty and \\frac{K}{L} \\rightarrow q \\in (0, \\infty) \\frac{K}{L} \\rightarrow q \\in (0, \\infty) . S_{A}^2 S_{A}^2 and S_{B}^2 S_{B}^2 are sample variances. Two sided asymptotic confidence interval for absolute difference equals to \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big), \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big), where u_{1 - \\alpha / 2} u_{1 - \\alpha / 2} is (1 - \\alpha / 2)- (1 - \\alpha / 2)- quantile of normal distribution \\mathcal{N}(0, 1) \\mathcal{N}(0, 1) . P-value equals to $$ p = 2 \\big(1 - \\Phi(|z|) \\big), $$ where \\Phi \\Phi is distribution function of \\mathcal{N}(0, 1) \\mathcal{N}(0, 1) and z z is observed value of test statistics Z_{K,L} Z_{K,L} . In practice is usually used Welch test , which uses t-distribution instead of normal distribution, with f f degrees of freedom given as f = \\frac{\\big( \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} \\big)^2}{\\frac{S_{A}^4}{K^2 (K-1)} + \\frac{S_{B}^4}{L^2 (L-1)}} . f = \\frac{\\big( \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} \\big)^2}{\\frac{S_{A}^4}{K^2 (K-1)} + \\frac{S_{B}^4}{L^2 (L-1)}} . Then two sided asymptotic confidence interval (with t-quantiles) for absolute difference equals to \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big). \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big). P-value equals to $$ p = 2 \\big(1 - \\text{CDF}_{t, f}(|z|) \\big), $$ where \\text{CDF}_{t,f} \\text{CDF}_{t,f} is cumulative distribution function of t-distribution with f f degrees of freedom and z z is observed value of test statistics Z_{K,L} Z_{K,L} .","title":"Absolute Difference"},{"location":"stats/ctr.html#relative-difference","text":"To derive asymptotic distribution for relative difference we will again use multivariate delta method with a link function g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} g: \\mathbb{R}^2 \\rightarrow \\mathbb{R} defined as g(x, y) = \\frac{y - x}{x} g(x, y) = \\frac{y - x}{x} . Be aware of order x x and y y . Gradient in point (\\mu_{A}, \\mu_{B}) (\\mu_{A}, \\mu_{B}) equals to \\nabla g (\\mu_{A}, \\mu_{B}) = \\big( -\\frac{\\mu_{B}}{\\mu_{A}^2}, \\frac{1}{\\mu_{A}} \\big). \\nabla g (\\mu_{A}, \\mu_{B}) = \\big( -\\frac{\\mu_{B}}{\\mu_{A}^2}, \\frac{1}{\\mu_{A}} \\big). The result is \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} \\Big) \\stackrel{as}{\\sim} \\mathcal{N} \\Big(0, \\, \\frac{1}{\\mu_{A}^2} \\big(\\frac{\\mu_{B}^2}{\\mu_{A}^2} \\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L} \\big)\\Big). \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} \\Big) \\stackrel{as}{\\sim} \\mathcal{N} \\Big(0, \\, \\frac{1}{\\mu_{A}^2} \\big(\\frac{\\mu_{B}^2}{\\mu_{A}^2} \\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L} \\big)\\Big). This can be rewritten in following form Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), if K, L \\rightarrow \\infty K, L \\rightarrow \\infty and \\frac{K}{L} \\rightarrow q \\in (0, \\infty) \\frac{K}{L} \\rightarrow q \\in (0, \\infty) . S_{A}^2 S_{A}^2 and S_{B}^2 S_{B}^2 are sample variances. For unknown true relative difference \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} we can derive confidence interval. For simplicity let's denote the sample variance as \\tilde{S}^2 \\tilde{S}^2 , i.e.: \\tilde{S}^2 = \\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }. \\tilde{S}^2 = \\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }. Finaly, the two sided asymptotic confidence interval for relative difference equals to \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 ; \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} + \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 \\Big). \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 ; \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} + \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 \\Big). P-value equals to $$ p = 2 \\big(1 - \\Phi(|z|) \\big), $$ where \\Phi \\Phi is distribution function of \\mathcal{N}(0, 1) \\mathcal{N}(0, 1) and z z is observed value of test statistics Z_{K,L} Z_{K,L} . Since we know, there is no straightforward approximation using t-quantiles, because there is no formula for degrees of freedom. In practise, we have huge amount of observations and both quantiles (normal and t-quantile) are very close to each other for large n n . A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas \u21a9","title":"Relative Difference"},{"location":"stats/multiple.html","text":"Multiple Comparison Correction \u00b6 Multiple comparisons problem is a problem, when we perform instead of one statistical test (i.e. difference between A and B variant) multiple statistical tests (i.e. difference between A and B, A and C, A and D). Some of them may have p-values less than 0.05 purely by chance, even if all our null hypotheses are really true. There exist various solutions. They differ mainly in power and implementation difficulty. We present one-by-one Bonferroni correction, HolmBonferroni method and Sidak correction. We summarize pros and cons of methods and provide step-by-step algorithms. Finally we present simple example in order to compare suggested methods. Generally HolmBonferroni method and the idk correction are universally more powerful procedures than the Bonferroni correction. Bonferroni correction is the most conservative one and suitable for independent tests. Summary \u00b6 After taking into account computational difficulty and results from a simple, but quite real example, we suggest to use Holm-Bonferroni method. Multiple Comparisons Problem \u00b6 Source Multiple comparisons problem (Wikipedia) . Let assume we want to test m m null hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Let denote by \\alpha \\alpha our overall level of significance. Typically we set \\alpha = 0.05 \\alpha = 0.05 . For each hull hypotheses we calculate p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} . We need to adjust these p-values for individual hypotheses H_{0}^{i} H_{0}^{i} , i = 1, \\dots, m i = 1, \\dots, m in order to satisfy overall level of significance \\alpha \\alpha . Bonferroni correction \u00b6 Source Bonferroni correction (Wikipedia) . This is the simplest way how to deal with Multiple comparisons problem. It is easy to implement, on the other hand, it is too conservative and tests loose too much power. This correction works in the worst-case scenario that all tests are independent. Since we test difference between A and B, A and C, A and D variant, these tests are dependent and therefore this correction is too stringent. Algorithm \u00b6 Compute p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Compute adjusted p-values using formula \\tilde{p}_{i} = \\min \\big\\{ m \\, p_{i}, 1 \\big\\}, \\,\\,\\, i = 1, \\dots, m \\tilde{p}_{i} = \\min \\big\\{ m \\, p_{i}, 1 \\big\\}, \\,\\,\\, i = 1, \\dots, m Reject null hypothesis H_{0}^{i} H_{0}^{i} if and only if \\tilde{p}_{i} \\leq \\alpha \\tilde{p}_{i} \\leq \\alpha . Example \u00b6 import statsmodels.stats.multitest pvals = [ 0.01 , 0.04 , 0.03 ] decision , adj_pvals , sidak_aplha , bonf_alpha = statsmodels . stats . multitest . multipletests ( pvals = pvals , alpha = 0.05 , method = 'bonferroni' ) print ( f 'Original p-values: \\t { pvals } ' ) print ( f 'Adjusted p-values: \\t { adj_pvals } ' ) Original p-values: [0.01, 0.04, 0.03] Adjusted p-values: [0.03 0.12 0.09] HolmBonferroni method \u00b6 Source HolmBonferroni method (Wikipedia) . Holm-Bonferroni method is more powerful than Bonferroni and it is valid under the same assumptions. Algorithm \u00b6 Compute p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Order p-values from lowest to highest p_{(1)} \\leq \\dots \\leq p_{(m)} p_{(1)} \\leq \\dots \\leq p_{(m)} Let k k be the minimal index such that p_{k} (m + 1 - k) > \\alpha p_{k} (m + 1 - k) > \\alpha . Reject the null hypotheses H_{0}^{(1)}, \\dots, H_{0}^{(k-1)} H_{0}^{(1)}, \\dots, H_{0}^{(k-1)} and do not reject H_{0}^{(k)}, \\dots, H_{0}^{(m)} H_{0}^{(k)}, \\dots, H_{0}^{(m)} . If k = 1 k = 1 then do not reject any of the null hypotheses and if no such k k index exist then reject all of the null hypotheses. Example \u00b6 decision , adj_pvals , sidak_aplha , bonf_alpha = statsmodels . stats . multitest . multipletests ( pvals = pvals , alpha = 0.05 , method = 'holm' ) print ( f 'Original p-values: \\t { pvals } ' ) print ( f 'Adjusted p-values: \\t { adj_pvals } ' ) Original p-values: [0.01, 0.04, 0.03] Adjusted p-values: [0.03 0.06 0.06] idk correction \u00b6 Source idk correction (Wikipedia) . idk correction keeps Type I error rate of exactly \\alpha \\alpha when the tests are independent from each other and all null hypotheses are true. It is less stringent than the Bonferroni correction, but only slightly. Algorithm (for \\alpha \\alpha ) \u00b6 Compute p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Reject null hypothesis H_{0}^{i} H_{0}^{i} if and only if associated p-value p_{i} p_{i} is lower or equal than \\alpha_{SID} = 1 - (1 - \\alpha)^{\\frac{1}{m}} \\alpha_{SID} = 1 - (1 - \\alpha)^{\\frac{1}{m}} . Algorithm (for p-values) \u00b6 Compute p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Order p-values from lowest to highest p_{(1)} \\leq \\dots \\leq p_{(m)} p_{(1)} \\leq \\dots \\leq p_{(m)} Calculate adjusted p-values \\tilde{p}_{(i)} \\tilde{p}_{(i)} using formula $$ \\tilde{p} {(i)}= \\begin{cases} 1 - (1 - p )^{m} & \\text{for i = 1 i = 1 },\\ \\max \\big{ \\tilde{p} {(i-1)}, 1 - (1 - p )^{m-i+1} \\big} & \\text{for i = 2, \\dots, m i = 2, \\dots, m }. \\end{cases} $$ Example \u00b6 decision , adj_pvals , sidak_aplha , bonf_alpha = statsmodels . stats . multitest . multipletests ( pvals = pvals , alpha = 0.05 , method = 'sidak' ) print ( f 'Original p-values: \\t { pvals } ' ) print ( f 'Adjusted p-values: \\t { adj_pvals } ' ) Original p-values: [0.01, 0.04, 0.03] Adjusted p-values: [0.029701 0.115264 0.087327] Confidence Interval Adjustment \u00b6 After p-values adjustment for Multiple comparisons problem it is also necessary to appropriately adjust confidence intervals. Fortunately this is an easy problem and it does not depend on chosen method. We use duality between p-values and confidence intervals. Algorithm \u00b6 Compute (original) p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Based on chosen method compute adjusted p-values \\tilde{p}_{1}, \\dots, \\tilde{p}_{m} \\tilde{p}_{1}, \\dots, \\tilde{p}_{m} . Compute adjustment ratios r_{1}, \\dots, r_{m} r_{1}, \\dots, r_{m} such that r_{i} = \\frac{p_{i}}{\\tilde{p}_{i}}, \\,\\,\\, i = 1, \\dots, m. r_{i} = \\frac{p_{i}}{\\tilde{p}_{i}}, \\,\\,\\, i = 1, \\dots, m. Let denote desired confidence level(s) as usual by \\alpha \\alpha , e.g. \\alpha = 0.05 \\alpha = 0.05 . More precisely let assume \\alpha_{1}, \\dots, \\alpha_{m} \\alpha_{1}, \\dots, \\alpha_{m} . Compute adjusted confidence levels \\tilde{\\alpha}_{1}, \\dots, \\tilde{\\alpha}_{m} \\tilde{\\alpha}_{1}, \\dots, \\tilde{\\alpha}_{m} such that \\tilde{\\alpha_{i}} = r \\, \\alpha_{i}. \\tilde{\\alpha_{i}} = r \\, \\alpha_{i}. Compute (1 - \\alpha_{i}) (1 - \\alpha_{i}) -confidence intervals such that you use \\tilde{\\alpha}_{i} \\tilde{\\alpha}_{i} instead of \\alpha_{i} \\alpha_{i} . Note Computed confidence intervals using \\tilde{\\alpha}_{i} \\tilde{\\alpha}_{i} instead of \\alpha_{i} \\alpha_{i} are actually (1 - \\alpha_{i}) (1 - \\alpha_{i}) -confidence intervals. Levels of significance DO NOT change, they are still 1 - \\alpha_{i} 1 - \\alpha_{i} , not 1 - \\tilde{\\alpha}_{i} 1 - \\tilde{\\alpha}_{i} . Example \u00b6 Unadjusted p-value is p_{i} = 0.01 p_{i} = 0.01 . Adjusted p-value is \\tilde{p}_{i} = 0.03 \\tilde{p}_{i} = 0.03 . Adjustment ratio is r = 1/3 r = 1/3 . Desired level of significance is \\alpha = 0.05 \\alpha = 0.05 . Adjusted level of significance is \\tilde{\\alpha} = 0.05 \\cdot 1/3 = 0.01667 \\tilde{\\alpha} = 0.05 \\cdot 1/3 = 0.01667 . So we need to compute 1 - 0.01667 = 0.983 = 98.3\\% 1 - 0.01667 = 0.983 = 98.3\\% -confidence interval in order to get 95\\% 95\\% -confidence interval. Python package \u00b6 Link to documentation of Statsmodels package .","title":"Multiple Comparison Correction"},{"location":"stats/multiple.html#multiple-comparison-correction","text":"Multiple comparisons problem is a problem, when we perform instead of one statistical test (i.e. difference between A and B variant) multiple statistical tests (i.e. difference between A and B, A and C, A and D). Some of them may have p-values less than 0.05 purely by chance, even if all our null hypotheses are really true. There exist various solutions. They differ mainly in power and implementation difficulty. We present one-by-one Bonferroni correction, HolmBonferroni method and Sidak correction. We summarize pros and cons of methods and provide step-by-step algorithms. Finally we present simple example in order to compare suggested methods. Generally HolmBonferroni method and the idk correction are universally more powerful procedures than the Bonferroni correction. Bonferroni correction is the most conservative one and suitable for independent tests.","title":"Multiple Comparison Correction"},{"location":"stats/multiple.html#summary","text":"After taking into account computational difficulty and results from a simple, but quite real example, we suggest to use Holm-Bonferroni method.","title":"Summary"},{"location":"stats/multiple.html#multiple-comparisons-problem","text":"Source Multiple comparisons problem (Wikipedia) . Let assume we want to test m m null hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Let denote by \\alpha \\alpha our overall level of significance. Typically we set \\alpha = 0.05 \\alpha = 0.05 . For each hull hypotheses we calculate p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} . We need to adjust these p-values for individual hypotheses H_{0}^{i} H_{0}^{i} , i = 1, \\dots, m i = 1, \\dots, m in order to satisfy overall level of significance \\alpha \\alpha .","title":"Multiple Comparisons Problem"},{"location":"stats/multiple.html#bonferroni-correction","text":"Source Bonferroni correction (Wikipedia) . This is the simplest way how to deal with Multiple comparisons problem. It is easy to implement, on the other hand, it is too conservative and tests loose too much power. This correction works in the worst-case scenario that all tests are independent. Since we test difference between A and B, A and C, A and D variant, these tests are dependent and therefore this correction is too stringent.","title":"Bonferroni correction"},{"location":"stats/multiple.html#algorithm","text":"Compute p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Compute adjusted p-values using formula \\tilde{p}_{i} = \\min \\big\\{ m \\, p_{i}, 1 \\big\\}, \\,\\,\\, i = 1, \\dots, m \\tilde{p}_{i} = \\min \\big\\{ m \\, p_{i}, 1 \\big\\}, \\,\\,\\, i = 1, \\dots, m Reject null hypothesis H_{0}^{i} H_{0}^{i} if and only if \\tilde{p}_{i} \\leq \\alpha \\tilde{p}_{i} \\leq \\alpha .","title":"Algorithm"},{"location":"stats/multiple.html#example","text":"import statsmodels.stats.multitest pvals = [ 0.01 , 0.04 , 0.03 ] decision , adj_pvals , sidak_aplha , bonf_alpha = statsmodels . stats . multitest . multipletests ( pvals = pvals , alpha = 0.05 , method = 'bonferroni' ) print ( f 'Original p-values: \\t { pvals } ' ) print ( f 'Adjusted p-values: \\t { adj_pvals } ' ) Original p-values: [0.01, 0.04, 0.03] Adjusted p-values: [0.03 0.12 0.09]","title":"Example"},{"location":"stats/multiple.html#holmbonferroni-method","text":"Source HolmBonferroni method (Wikipedia) . Holm-Bonferroni method is more powerful than Bonferroni and it is valid under the same assumptions.","title":"HolmBonferroni method"},{"location":"stats/multiple.html#algorithm_1","text":"Compute p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Order p-values from lowest to highest p_{(1)} \\leq \\dots \\leq p_{(m)} p_{(1)} \\leq \\dots \\leq p_{(m)} Let k k be the minimal index such that p_{k} (m + 1 - k) > \\alpha p_{k} (m + 1 - k) > \\alpha . Reject the null hypotheses H_{0}^{(1)}, \\dots, H_{0}^{(k-1)} H_{0}^{(1)}, \\dots, H_{0}^{(k-1)} and do not reject H_{0}^{(k)}, \\dots, H_{0}^{(m)} H_{0}^{(k)}, \\dots, H_{0}^{(m)} . If k = 1 k = 1 then do not reject any of the null hypotheses and if no such k k index exist then reject all of the null hypotheses.","title":"Algorithm"},{"location":"stats/multiple.html#example_1","text":"decision , adj_pvals , sidak_aplha , bonf_alpha = statsmodels . stats . multitest . multipletests ( pvals = pvals , alpha = 0.05 , method = 'holm' ) print ( f 'Original p-values: \\t { pvals } ' ) print ( f 'Adjusted p-values: \\t { adj_pvals } ' ) Original p-values: [0.01, 0.04, 0.03] Adjusted p-values: [0.03 0.06 0.06]","title":"Example"},{"location":"stats/multiple.html#idk-correction","text":"Source idk correction (Wikipedia) . idk correction keeps Type I error rate of exactly \\alpha \\alpha when the tests are independent from each other and all null hypotheses are true. It is less stringent than the Bonferroni correction, but only slightly.","title":"idk correction"},{"location":"stats/multiple.html#algorithm-for-alphaalpha","text":"Compute p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Reject null hypothesis H_{0}^{i} H_{0}^{i} if and only if associated p-value p_{i} p_{i} is lower or equal than \\alpha_{SID} = 1 - (1 - \\alpha)^{\\frac{1}{m}} \\alpha_{SID} = 1 - (1 - \\alpha)^{\\frac{1}{m}} .","title":"Algorithm (for \\alpha\\alpha)"},{"location":"stats/multiple.html#algorithm-for-p-values","text":"Compute p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Order p-values from lowest to highest p_{(1)} \\leq \\dots \\leq p_{(m)} p_{(1)} \\leq \\dots \\leq p_{(m)} Calculate adjusted p-values \\tilde{p}_{(i)} \\tilde{p}_{(i)} using formula $$ \\tilde{p} {(i)}= \\begin{cases} 1 - (1 - p )^{m} & \\text{for i = 1 i = 1 },\\ \\max \\big{ \\tilde{p} {(i-1)}, 1 - (1 - p )^{m-i+1} \\big} & \\text{for i = 2, \\dots, m i = 2, \\dots, m }. \\end{cases} $$","title":"Algorithm (for p-values)"},{"location":"stats/multiple.html#example_2","text":"decision , adj_pvals , sidak_aplha , bonf_alpha = statsmodels . stats . multitest . multipletests ( pvals = pvals , alpha = 0.05 , method = 'sidak' ) print ( f 'Original p-values: \\t { pvals } ' ) print ( f 'Adjusted p-values: \\t { adj_pvals } ' ) Original p-values: [0.01, 0.04, 0.03] Adjusted p-values: [0.029701 0.115264 0.087327]","title":"Example"},{"location":"stats/multiple.html#confidence-interval-adjustment","text":"After p-values adjustment for Multiple comparisons problem it is also necessary to appropriately adjust confidence intervals. Fortunately this is an easy problem and it does not depend on chosen method. We use duality between p-values and confidence intervals.","title":"Confidence Interval Adjustment"},{"location":"stats/multiple.html#algorithm_2","text":"Compute (original) p-values p_{1}, \\dots, p_{m} p_{1}, \\dots, p_{m} for all m m hypotheses H_{0}^{1}, \\dots, H_{0}^{m} H_{0}^{1}, \\dots, H_{0}^{m} . Based on chosen method compute adjusted p-values \\tilde{p}_{1}, \\dots, \\tilde{p}_{m} \\tilde{p}_{1}, \\dots, \\tilde{p}_{m} . Compute adjustment ratios r_{1}, \\dots, r_{m} r_{1}, \\dots, r_{m} such that r_{i} = \\frac{p_{i}}{\\tilde{p}_{i}}, \\,\\,\\, i = 1, \\dots, m. r_{i} = \\frac{p_{i}}{\\tilde{p}_{i}}, \\,\\,\\, i = 1, \\dots, m. Let denote desired confidence level(s) as usual by \\alpha \\alpha , e.g. \\alpha = 0.05 \\alpha = 0.05 . More precisely let assume \\alpha_{1}, \\dots, \\alpha_{m} \\alpha_{1}, \\dots, \\alpha_{m} . Compute adjusted confidence levels \\tilde{\\alpha}_{1}, \\dots, \\tilde{\\alpha}_{m} \\tilde{\\alpha}_{1}, \\dots, \\tilde{\\alpha}_{m} such that \\tilde{\\alpha_{i}} = r \\, \\alpha_{i}. \\tilde{\\alpha_{i}} = r \\, \\alpha_{i}. Compute (1 - \\alpha_{i}) (1 - \\alpha_{i}) -confidence intervals such that you use \\tilde{\\alpha}_{i} \\tilde{\\alpha}_{i} instead of \\alpha_{i} \\alpha_{i} . Note Computed confidence intervals using \\tilde{\\alpha}_{i} \\tilde{\\alpha}_{i} instead of \\alpha_{i} \\alpha_{i} are actually (1 - \\alpha_{i}) (1 - \\alpha_{i}) -confidence intervals. Levels of significance DO NOT change, they are still 1 - \\alpha_{i} 1 - \\alpha_{i} , not 1 - \\tilde{\\alpha}_{i} 1 - \\tilde{\\alpha}_{i} .","title":"Algorithm"},{"location":"stats/multiple.html#example_3","text":"Unadjusted p-value is p_{i} = 0.01 p_{i} = 0.01 . Adjusted p-value is \\tilde{p}_{i} = 0.03 \\tilde{p}_{i} = 0.03 . Adjustment ratio is r = 1/3 r = 1/3 . Desired level of significance is \\alpha = 0.05 \\alpha = 0.05 . Adjusted level of significance is \\tilde{\\alpha} = 0.05 \\cdot 1/3 = 0.01667 \\tilde{\\alpha} = 0.05 \\cdot 1/3 = 0.01667 . So we need to compute 1 - 0.01667 = 0.983 = 98.3\\% 1 - 0.01667 = 0.983 = 98.3\\% -confidence interval in order to get 95\\% 95\\% -confidence interval.","title":"Example"},{"location":"stats/multiple.html#python-package","text":"Link to documentation of Statsmodels package .","title":"Python package"},{"location":"stats/sample_size.html","text":"Sample Size \u00b6 Setting a correct sample size is a key step in launching successful experiments. We discuss what has an effect on required experiment sample size, how to calculate it and how to proceed if required sample size is too big. There is no free lunch in experimenting, everyone doing experiments navigates between trade-offs of significance, baseline metric conversion rate, measured differences and sample size. What Impacts Sample Size \u00b6 Sample size depends on selected primary metric, on its baseline conversion rate and on a size of minimal detectable effect (MDE) . MDE is a minimal relative difference in primary metric between treatment and control variant, which we are willing to detect, i.e. which will be statistically significant. One way how to look at MDE is as experimenters estimate size of the impact the change in the experiment will have. But there's more practical view of MDE. While it is always good to run an experiment that detects +1% change as statistically significant, it does not make always sense to full-scale such experiment. The change of +1% is simply too small so it does not have business impact offsetting the cost of full-scaled solution. Moreover to detect small change such as +1%, you need to have a great amount of users in the experiment. Let assume we full-scale the experiment if the effect on primary metric is at least +5% . Then +5% is our MDE. Detecting +5% effect as statistically significant requires much less users than detecting +1% change. This gives us also a way how to estimate MDEs in different experiments. MDE is a minimal change the experimenter wants to measure in the experiment to make it reasonable to full-scale. Small Baseline \u00b6 Sales metrics like Conversions per Pageview or Conversions per User have usually baseline conversion rates below 1%. This requires to use big sample sizes to measure significant results. Following graph shows how sample size depends on various smaller baseline conversion rates and MDE. We can see that we need over 20,000 samples per variant to measure significant difference of 20% on a metric with 1% baseline conversion rate. Bigger Baseline \u00b6 Engagement metrics like Click-through Rate (CTR) or proxy metrics usually have much higher rates than sales metrics making it easier to measure significant differences. We can see that we need only 3,000 samples per variant to detect a difference of 5% as statistically significant on a metric with 50% baseline conversion rate. Calculating Sample Size \u00b6 We can calculate sample size n n per variant as: n = \\frac{(z_{\\alpha/2})^2\\left[\\hat{p}_A(1-\\hat{p}_A) + \\hat{p}_B(1-\\hat{p}_B)\\right]}{\\Delta^2} n = \\frac{(z_{\\alpha/2})^2\\left[\\hat{p}_A(1-\\hat{p}_A) + \\hat{p}_B(1-\\hat{p}_B)\\right]}{\\Delta^2} where z_{\\alpha/2} z_{\\alpha/2} is a critical value of normal distribution for confidence level \\alpha \\alpha , \\hat{p}_A \\hat{p}_A is baseline conversion rate in variant A, \\hat{p}_B \\hat{p}_B is expected or estimated conversion rate in variant B and \\Delta = \\hat{p}_B - \\hat{p}_A \\Delta = \\hat{p}_B - \\hat{p}_A is minimal detectable effect (MDE). Note For conversion rates, sample variance \\hat{\\sigma}^2 = \\hat{p}(1-\\hat{p})/n \\hat{\\sigma}^2 = \\hat{p}(1-\\hat{p})/n . As we discussed in previous chapter, we can see that: Sample size increases as MDE decreases (because \\Delta \\Delta is in the denominator). Sample size is greatest for conversion rates equal to 0.5. What to Do If Sample Size is Too Big \u00b6 We saw that required sample size is Proportional to \\hat{\\sigma}^2/\\Delta^2 \\hat{\\sigma}^2/\\Delta^2 . Many key metrics have high-variance (eg. Conversions per User, Revenue per Mile (RPM), ... The problem: As we optimize, we need to measure smaller and smaller differences. We need 100K users per variant to detect 2% change to RPM. We need 40M users to detect 0.1% change to RPM What can we do when required sample size is too big to run reasonable experiment? Proxy Metrics \u00b6 Is there anything else in users' interactions that tells us what variant they like more? Do users click more? Do users return more often? Do users return quicker? How long do users spend on our website? How long do users spend on the particular page? We can measure proxy metrics for every exposure of every user and every session, engagement metrics have much higher baseline conversion rates thus require much smaller sample sizes. Engagement metrics usually correlate with sales metrics e.g. who clicks also buys etc. To show 10% improvement on 50% baseline we need 764 samples! There are not many engagement metrics 1 available in EP at the moment, we would like to add some from the following list. At the moment, please, take it as an inspiration of what will be possible in the future. Metric Experimentation Unit Description User metrics Sessions per User User Clicks per User User Clicks on any element on the website, user-level CTR. Duration metrics Duration of a Session Session Sessions Duration per User User Duration of an Absence Absence-2 Duration of an Absence per User User-2 If user had at least 2 sessions Time on Page Pageview On experiment screen Time to Click Pageview On experiment screen or experiment element only Action metrics CTR Pageview On experiment screen or experiment element only Metrics with too low conversion rate Downloads Pageview RPM Pageview On experiment screen or experiment element only CNV Pageview On experiment screen or experiment element only Downloads per User User Transactions per User User Yandex, Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics \u21a9","title":"Sample Size"},{"location":"stats/sample_size.html#sample-size","text":"Setting a correct sample size is a key step in launching successful experiments. We discuss what has an effect on required experiment sample size, how to calculate it and how to proceed if required sample size is too big. There is no free lunch in experimenting, everyone doing experiments navigates between trade-offs of significance, baseline metric conversion rate, measured differences and sample size.","title":"Sample Size"},{"location":"stats/sample_size.html#what-impacts-sample-size","text":"Sample size depends on selected primary metric, on its baseline conversion rate and on a size of minimal detectable effect (MDE) . MDE is a minimal relative difference in primary metric between treatment and control variant, which we are willing to detect, i.e. which will be statistically significant. One way how to look at MDE is as experimenters estimate size of the impact the change in the experiment will have. But there's more practical view of MDE. While it is always good to run an experiment that detects +1% change as statistically significant, it does not make always sense to full-scale such experiment. The change of +1% is simply too small so it does not have business impact offsetting the cost of full-scaled solution. Moreover to detect small change such as +1%, you need to have a great amount of users in the experiment. Let assume we full-scale the experiment if the effect on primary metric is at least +5% . Then +5% is our MDE. Detecting +5% effect as statistically significant requires much less users than detecting +1% change. This gives us also a way how to estimate MDEs in different experiments. MDE is a minimal change the experimenter wants to measure in the experiment to make it reasonable to full-scale.","title":"What Impacts Sample Size"},{"location":"stats/sample_size.html#small-baseline","text":"Sales metrics like Conversions per Pageview or Conversions per User have usually baseline conversion rates below 1%. This requires to use big sample sizes to measure significant results. Following graph shows how sample size depends on various smaller baseline conversion rates and MDE. We can see that we need over 20,000 samples per variant to measure significant difference of 20% on a metric with 1% baseline conversion rate.","title":"Small Baseline"},{"location":"stats/sample_size.html#bigger-baseline","text":"Engagement metrics like Click-through Rate (CTR) or proxy metrics usually have much higher rates than sales metrics making it easier to measure significant differences. We can see that we need only 3,000 samples per variant to detect a difference of 5% as statistically significant on a metric with 50% baseline conversion rate.","title":"Bigger Baseline"},{"location":"stats/sample_size.html#calculating-sample-size","text":"We can calculate sample size n n per variant as: n = \\frac{(z_{\\alpha/2})^2\\left[\\hat{p}_A(1-\\hat{p}_A) + \\hat{p}_B(1-\\hat{p}_B)\\right]}{\\Delta^2} n = \\frac{(z_{\\alpha/2})^2\\left[\\hat{p}_A(1-\\hat{p}_A) + \\hat{p}_B(1-\\hat{p}_B)\\right]}{\\Delta^2} where z_{\\alpha/2} z_{\\alpha/2} is a critical value of normal distribution for confidence level \\alpha \\alpha , \\hat{p}_A \\hat{p}_A is baseline conversion rate in variant A, \\hat{p}_B \\hat{p}_B is expected or estimated conversion rate in variant B and \\Delta = \\hat{p}_B - \\hat{p}_A \\Delta = \\hat{p}_B - \\hat{p}_A is minimal detectable effect (MDE). Note For conversion rates, sample variance \\hat{\\sigma}^2 = \\hat{p}(1-\\hat{p})/n \\hat{\\sigma}^2 = \\hat{p}(1-\\hat{p})/n . As we discussed in previous chapter, we can see that: Sample size increases as MDE decreases (because \\Delta \\Delta is in the denominator). Sample size is greatest for conversion rates equal to 0.5.","title":"Calculating Sample Size"},{"location":"stats/sample_size.html#what-to-do-if-sample-size-is-too-big","text":"We saw that required sample size is Proportional to \\hat{\\sigma}^2/\\Delta^2 \\hat{\\sigma}^2/\\Delta^2 . Many key metrics have high-variance (eg. Conversions per User, Revenue per Mile (RPM), ... The problem: As we optimize, we need to measure smaller and smaller differences. We need 100K users per variant to detect 2% change to RPM. We need 40M users to detect 0.1% change to RPM What can we do when required sample size is too big to run reasonable experiment?","title":"What to Do If Sample Size is Too Big"},{"location":"stats/sample_size.html#proxy-metrics","text":"Is there anything else in users' interactions that tells us what variant they like more? Do users click more? Do users return more often? Do users return quicker? How long do users spend on our website? How long do users spend on the particular page? We can measure proxy metrics for every exposure of every user and every session, engagement metrics have much higher baseline conversion rates thus require much smaller sample sizes. Engagement metrics usually correlate with sales metrics e.g. who clicks also buys etc. To show 10% improvement on 50% baseline we need 764 samples! There are not many engagement metrics 1 available in EP at the moment, we would like to add some from the following list. At the moment, please, take it as an inspiration of what will be possible in the future. Metric Experimentation Unit Description User metrics Sessions per User User Clicks per User User Clicks on any element on the website, user-level CTR. Duration metrics Duration of a Session Session Sessions Duration per User User Duration of an Absence Absence-2 Duration of an Absence per User User-2 If user had at least 2 sessions Time on Page Pageview On experiment screen Time to Click Pageview On experiment screen or experiment element only Action metrics CTR Pageview On experiment screen or experiment element only Metrics with too low conversion rate Downloads Pageview RPM Pageview On experiment screen or experiment element only CNV Pageview On experiment screen or experiment element only Downloads per User User Transactions per User User Yandex, Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics \u21a9","title":"Proxy Metrics"},{"location":"stats/sequential.html","text":"Sequential Analysis \u00b6 There is a strong demand from experimenters to statisticians for support of online decision making in online controlled experiments (OCE). Given big volume of observations and real-time nature of modern data pipelines, this demand is validated. This brings new issues to statistical evaluations. We cannot use standard Student\u2019s t-test, because it assumes that we evaluate data only once - after all experiment data has been collected. It has been known for a while, that evaluating experiments sequentially using standard t-tests increases dramatically type I errors (False-positive errors) that platforms try to keep below desired 5%. Using standard t-tests sequentially increases false-positive errors to a range of 20-30% or even higher making whole statistical inference useless. We verified this statement using simulations of A/A tests. We can evaluate experiments sequentially without exceeding desired false-positive errors using more sophisticated methods. Unfortunately, there is no unique approach how to deal with this problem. Multiple approaches differs mainly in mathematical complexity. Therefore we present a very basic one, so-called Alpha Spending Function 1 . Despite its simplicity, it had been used in clinical trials for decades, so we have a good reason to use it too. Peeking at Temporary Results is Wrong \u00b6 We can say that any A/A experiment when ran long enough will show significant difference in the primary metric regardless of the validity of the null hypothesis. So there's 100% chance that we get false-positive result if we are not careful about this fact. We want to keep this error below 5% any-time. This is not possible without setting experiment duration before starting it. Knowing where we are in the period of experiment helps EP to provide results with controlled false-positive error rate any time. Following graph illustrates this problem. Test statistic measures difference in value of treatment variant metric to the value of control variant on unit scale. If we call the experiment off once the value of the test statistics crosses one of two constant boundaries (dashed lines) for the first time at day 6, we might be doing false-positive error compared to if we evaluated the experiment once completed at day 14. Alpha Spending Function \u00b6 EP allows for safe early-stopping when observed difference is big enough. It requires the difference to be bigger early in the experiment than at the end of the experiment. Intuition is that if the experiment is doing really great or really poor, we do not need to wait full experiment duration to make a decision. Following graph shows decreasing decision boundary that requires observed difference to be big at the beginning of the experiment and decreases along the time. The decision boundary is called Alpha Spending Function and we implemented O'Brien-Fleming 2 version of it. This process ensures below 5% false-positive errors at any time during the experiment but requires setting experiment duration upfront to be able to calculate decreasing boundaries. This requirement is covered in implementation of the experiment protocol . Simulation \u00b6 We simulate 2000 A/A experiments for Click-through rate metric. We want to compare three possible solutions to sequential evaluation: Old-school - we evaluate the experiment once and after all data has been collected. p-Value hacking or peeking - we evaluate the experiment once a day during whole experiment duration using constant decision boundaries. Sequential experimenting - we use alpha spending function and evaluate once a day during whole experiment duration. We measure the quality of the solution by false-positive error rate. Following chart depicts development of A/A experiments in 14 days. We see that old-school and sequential experimenting methods keep false-positives below 5% while p-value hacking method shows false-positive errors around 20%. The longer we run the experiment the greater false-positive error rate we can expect. It can even be 29% for 30-day experiment. We can conclude that sequential experimenting is the best method for our needs and simple enough to implement. David L. DeMents, K. K. Gordon Lan, Interim analysis: The alpha spending function approach \u21a9 P. C. O\u2019Brien, T.R. Fleming - A Multiple Testing Procedure for Clinical Trials \u21a9","title":"Sequential Evaluation"},{"location":"stats/sequential.html#sequential-analysis","text":"There is a strong demand from experimenters to statisticians for support of online decision making in online controlled experiments (OCE). Given big volume of observations and real-time nature of modern data pipelines, this demand is validated. This brings new issues to statistical evaluations. We cannot use standard Student\u2019s t-test, because it assumes that we evaluate data only once - after all experiment data has been collected. It has been known for a while, that evaluating experiments sequentially using standard t-tests increases dramatically type I errors (False-positive errors) that platforms try to keep below desired 5%. Using standard t-tests sequentially increases false-positive errors to a range of 20-30% or even higher making whole statistical inference useless. We verified this statement using simulations of A/A tests. We can evaluate experiments sequentially without exceeding desired false-positive errors using more sophisticated methods. Unfortunately, there is no unique approach how to deal with this problem. Multiple approaches differs mainly in mathematical complexity. Therefore we present a very basic one, so-called Alpha Spending Function 1 . Despite its simplicity, it had been used in clinical trials for decades, so we have a good reason to use it too.","title":"Sequential Analysis"},{"location":"stats/sequential.html#peeking-at-temporary-results-is-wrong","text":"We can say that any A/A experiment when ran long enough will show significant difference in the primary metric regardless of the validity of the null hypothesis. So there's 100% chance that we get false-positive result if we are not careful about this fact. We want to keep this error below 5% any-time. This is not possible without setting experiment duration before starting it. Knowing where we are in the period of experiment helps EP to provide results with controlled false-positive error rate any time. Following graph illustrates this problem. Test statistic measures difference in value of treatment variant metric to the value of control variant on unit scale. If we call the experiment off once the value of the test statistics crosses one of two constant boundaries (dashed lines) for the first time at day 6, we might be doing false-positive error compared to if we evaluated the experiment once completed at day 14.","title":"Peeking at Temporary Results is Wrong"},{"location":"stats/sequential.html#alpha-spending-function","text":"EP allows for safe early-stopping when observed difference is big enough. It requires the difference to be bigger early in the experiment than at the end of the experiment. Intuition is that if the experiment is doing really great or really poor, we do not need to wait full experiment duration to make a decision. Following graph shows decreasing decision boundary that requires observed difference to be big at the beginning of the experiment and decreases along the time. The decision boundary is called Alpha Spending Function and we implemented O'Brien-Fleming 2 version of it. This process ensures below 5% false-positive errors at any time during the experiment but requires setting experiment duration upfront to be able to calculate decreasing boundaries. This requirement is covered in implementation of the experiment protocol .","title":"Alpha Spending Function"},{"location":"stats/sequential.html#simulation","text":"We simulate 2000 A/A experiments for Click-through rate metric. We want to compare three possible solutions to sequential evaluation: Old-school - we evaluate the experiment once and after all data has been collected. p-Value hacking or peeking - we evaluate the experiment once a day during whole experiment duration using constant decision boundaries. Sequential experimenting - we use alpha spending function and evaluate once a day during whole experiment duration. We measure the quality of the solution by false-positive error rate. Following chart depicts development of A/A experiments in 14 days. We see that old-school and sequential experimenting methods keep false-positives below 5% while p-value hacking method shows false-positive errors around 20%. The longer we run the experiment the greater false-positive error rate we can expect. It can even be 29% for 30-day experiment. We can conclude that sequential experimenting is the best method for our needs and simple enough to implement. David L. DeMents, K. K. Gordon Lan, Interim analysis: The alpha spending function approach \u21a9 P. C. O\u2019Brien, T.R. Fleming - A Multiple Testing Procedure for Clinical Trials \u21a9","title":"Simulation"},{"location":"user_guide/aggregation.html","text":"Aggregation \u00b6 Goals in Metric Definition \u00b6 Goals in the metric definition look like value(test_unit_type.unit.conversion) where count , value , or unique determines we need number of goals recorded or their value (e.g. USD bookings of conversion goal) or that we need only info if at least 1 goal has been collected. test_unit_type is a type of the unit the goal has been recorded for. unit or global are types of aggregation. conversion is a name of the goal. Supported Unit Types \u00b6 Ep-stats support any type of randomization unit. It is a responsibility of an integrator to correctly query for the data. Note on Randomization \u00b6 It is necessary for the statistics to work correctly that unit exposures are randomly (independently and identically a.k.a. IID) distributed within one experiment into its variants. This is usually the case when we randomize at pageview or event session unit types. In general, one unit can experience the experiment variant multiple times. Violation of IID leads to uncontrolled false-positive errors in metric evaluation. Ep-stats remedies this IID violation by using delta method for IID . Aggregation Types \u00b6 There are 2 types or levels of aggregation of goals available: global aggregates goals as if one goal is one observation. unit aggregates goals first per unit (e.g. count of conversion goals per unit id unit_1 ). Unit Aggregation Type \u00b6 We need to use unit aggregation type when we calculate any \"per User\" or generally any \"per exposure\" metrics. It is \"per User\" metric so we need all goals that happened for one user represented as one observation of one unit. This is required for correct calculation of sample standard deviation which is a basic block in all statistical evaluation methods. For example, if there are 2 conversion goals for unit id unit_1 , we want to have 2 as a count of conversion goals for this unit id making it 1 observation rather than having 2 separate observations of conversion goals. Global Aggregation Type \u00b6 Global aggregation type skips \"per User\" aggregation step described in previous section and treats every goal as one observation. This is now enough for all metrics that are not based directly on exposures of the experiment randomization unit type. For example Refund Rate metric defined as count(test_unit_type.global.refund) / count(test_unit_type.global.conversion) does not need to use unit aggregation type because 1 observation is 1 conversion that can have only zero or one refund goal. This kind of metrics needs application of delta method or bootstrapping which has not been implemented yet. Dimensional Goals \u00b6 Metric goal definition allows to filter goals by supported dimensions. For example we can specify that we want transactional metrics to be calculated only for transactions involving VPN products. Or we want Revenue per Mile or CTR metrics calculated for particular screen. For example Average Bookings of product p_1 can be defined as value(test_unit_type.unit.conversion(product=p_1)) / count(test_unit_type.global.exposure) Different goals may allow filtering by different dimensions. Note Dimensions are not supported yet. Example \u00b6 See Test Data for examples of pre-aggregated goals that make input to statistical evaluation using Experiment.evaluate_agg or per unit goals that make input to statistical evaluation using Experiment.evaluate_by_unit .","title":"Aggregation"},{"location":"user_guide/aggregation.html#aggregation","text":"","title":"Aggregation"},{"location":"user_guide/aggregation.html#goals-in-metric-definition","text":"Goals in the metric definition look like value(test_unit_type.unit.conversion) where count , value , or unique determines we need number of goals recorded or their value (e.g. USD bookings of conversion goal) or that we need only info if at least 1 goal has been collected. test_unit_type is a type of the unit the goal has been recorded for. unit or global are types of aggregation. conversion is a name of the goal.","title":"Goals in Metric Definition"},{"location":"user_guide/aggregation.html#supported-unit-types","text":"Ep-stats support any type of randomization unit. It is a responsibility of an integrator to correctly query for the data.","title":"Supported Unit Types"},{"location":"user_guide/aggregation.html#note-on-randomization","text":"It is necessary for the statistics to work correctly that unit exposures are randomly (independently and identically a.k.a. IID) distributed within one experiment into its variants. This is usually the case when we randomize at pageview or event session unit types. In general, one unit can experience the experiment variant multiple times. Violation of IID leads to uncontrolled false-positive errors in metric evaluation. Ep-stats remedies this IID violation by using delta method for IID .","title":"Note on Randomization"},{"location":"user_guide/aggregation.html#aggregation-types","text":"There are 2 types or levels of aggregation of goals available: global aggregates goals as if one goal is one observation. unit aggregates goals first per unit (e.g. count of conversion goals per unit id unit_1 ).","title":"Aggregation Types"},{"location":"user_guide/aggregation.html#unit-aggregation-type","text":"We need to use unit aggregation type when we calculate any \"per User\" or generally any \"per exposure\" metrics. It is \"per User\" metric so we need all goals that happened for one user represented as one observation of one unit. This is required for correct calculation of sample standard deviation which is a basic block in all statistical evaluation methods. For example, if there are 2 conversion goals for unit id unit_1 , we want to have 2 as a count of conversion goals for this unit id making it 1 observation rather than having 2 separate observations of conversion goals.","title":"Unit Aggregation Type"},{"location":"user_guide/aggregation.html#global-aggregation-type","text":"Global aggregation type skips \"per User\" aggregation step described in previous section and treats every goal as one observation. This is now enough for all metrics that are not based directly on exposures of the experiment randomization unit type. For example Refund Rate metric defined as count(test_unit_type.global.refund) / count(test_unit_type.global.conversion) does not need to use unit aggregation type because 1 observation is 1 conversion that can have only zero or one refund goal. This kind of metrics needs application of delta method or bootstrapping which has not been implemented yet.","title":"Global Aggregation Type"},{"location":"user_guide/aggregation.html#dimensional-goals","text":"Metric goal definition allows to filter goals by supported dimensions. For example we can specify that we want transactional metrics to be calculated only for transactions involving VPN products. Or we want Revenue per Mile or CTR metrics calculated for particular screen. For example Average Bookings of product p_1 can be defined as value(test_unit_type.unit.conversion(product=p_1)) / count(test_unit_type.global.exposure) Different goals may allow filtering by different dimensions. Note Dimensions are not supported yet.","title":"Dimensional Goals"},{"location":"user_guide/aggregation.html#example","text":"See Test Data for examples of pre-aggregated goals that make input to statistical evaluation using Experiment.evaluate_agg or per unit goals that make input to statistical evaluation using Experiment.evaluate_by_unit .","title":"Example"},{"location":"user_guide/configuring_api.html","text":"Integration \u00b6 This short integration guide assumes you are familiar with Basic Principles . Access to Data \u00b6 To evaluate a metric in experiment, we have to compile metric definition that comes in form of nominator and denominator expressions into some underlying data source in the form that is vastly company or use-case specific. We use class DAO to interface underlying data source. DAO gets all the information contained in Experiment and needs to compile it into SQL or something else understandable by company's data systems to provide pre-aggregated or by-unit goals. Following snippet shows one way how to aggregate data to provide input in form of pre-aggregated goals in some implementation of DAO class. SELECT -- we aggregate secondly by all dims required by ep-stats and omit `unit_id` -- this way we get correct $\\sum x^2$ values in `sum_sqr_value` to calculate -- correct sample standard deviation of real-valued metrics. exp_id , exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value , SUM ( sum_cnt ) count , SUM ( sum_cnt * sum_cnt ) sum_sqr_count , SUM ( value ) sum_value , SUM ( value * value ) sum_sqr_value , SUM ( unique ) count_unique FROM ( -- we aggregate firstly by all dims required by ep-stats and by `unit_id` SELECT exp_id , exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value , unit_id , SUM ( cnt ) sum_cnt , SUM ( value ) value , IF ( SUM ( cnt ) > 0 , 1 , 0 ) unique FROM events GROUP BY exp_id , exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value , unit_id ) u GROUP BY exp_id , exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value Configuring REST API \u00b6 After having access to our data in custom implementation of Dao class e.g. CustomDao , we can follow up an example in main.py to configure the REST API with our CustomDao . We need to implement CustomDaoFactory that creates instances of our CustomDao for every request served. We can then customize get_dao_factory() method in main.py and to launch the server. def get_dao_factory (): return CustomDaoFactory ( ... ) def main (): from .config import config logging . config . dictConfig ( config [ 'logging' ]) serve ( 'my_package:api' , settings . api , config [ 'logging' ]) if __name__ == '__main__' : main ()","title":"Configuring API"},{"location":"user_guide/configuring_api.html#integration","text":"This short integration guide assumes you are familiar with Basic Principles .","title":"Integration"},{"location":"user_guide/configuring_api.html#access-to-data","text":"To evaluate a metric in experiment, we have to compile metric definition that comes in form of nominator and denominator expressions into some underlying data source in the form that is vastly company or use-case specific. We use class DAO to interface underlying data source. DAO gets all the information contained in Experiment and needs to compile it into SQL or something else understandable by company's data systems to provide pre-aggregated or by-unit goals. Following snippet shows one way how to aggregate data to provide input in form of pre-aggregated goals in some implementation of DAO class. SELECT -- we aggregate secondly by all dims required by ep-stats and omit `unit_id` -- this way we get correct $\\sum x^2$ values in `sum_sqr_value` to calculate -- correct sample standard deviation of real-valued metrics. exp_id , exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value , SUM ( sum_cnt ) count , SUM ( sum_cnt * sum_cnt ) sum_sqr_count , SUM ( value ) sum_value , SUM ( value * value ) sum_sqr_value , SUM ( unique ) count_unique FROM ( -- we aggregate firstly by all dims required by ep-stats and by `unit_id` SELECT exp_id , exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value , unit_id , SUM ( cnt ) sum_cnt , SUM ( value ) value , IF ( SUM ( cnt ) > 0 , 1 , 0 ) unique FROM events GROUP BY exp_id , exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value , unit_id ) u GROUP BY exp_id , exp_variant_id , unit_type , agg_type , goal , dimension , dimension_value","title":"Access to Data"},{"location":"user_guide/configuring_api.html#configuring-rest-api","text":"After having access to our data in custom implementation of Dao class e.g. CustomDao , we can follow up an example in main.py to configure the REST API with our CustomDao . We need to implement CustomDaoFactory that creates instances of our CustomDao for every request served. We can then customize get_dao_factory() method in main.py and to launch the server. def get_dao_factory (): return CustomDaoFactory ( ... ) def main (): from .config import config logging . config . dictConfig ( config [ 'logging' ]) serve ( 'my_package:api' , settings . api , config [ 'logging' ]) if __name__ == '__main__' : main ()","title":"Configuring REST API"},{"location":"user_guide/protocol.html","text":"Experimentation Protocol \u00b6 We try to be well aware that implementation of experimentation protocol is what all of us experimenters will find the most difficult to understand and get done correctly when running experiments in teams and scaling the experimentation up to the whole companies. The experimentation protocol we are implementing is here to rather enable us to safely run and evaluate more experiments than to limit us from expanding our experimentation. Experimentation is a work with uncertainty where errors and erroneous decisions were and will be a necessary part of it. When experimenting on a larger scale, errors happen. It\u2019s no longer a \u201csmall chance of error\u201d in case when I am running one or two experiments. Errors happen with a specific chance. This chance must be monitored and controlled for. It will always be true that we will be making wrong decisions based on experiment data. There is no way out of it. We can only control it. It is not about being statistically correct, it is that we can and should know that we are making the right decision 95 times and the wrong one 5 times out of every 100 decisions we make. Not knowing and controlling for this, we are playing a game where we do not know our odds of a good or a bad decision. Only by joint effort among systems, data, and experimenters, we can guarantee to keep erroneous decisions at bay. Decisions we make are the most important outcome from experimenting, we need to keep bad decisions at bay. To do that, we need to set experiment duration upfront and follow the experimentation protocol. Following experimentation protocol is not special to us, it is a proven concept from clinical trials and also from online controlled experiments, it ensures that we are making the right decisions most of the time. Setting experiment duration upfront is certainly the most difficult point. We are not sure if current implementation (still in development) is the best one, we will need to iterate based on your feedback. We will have a calculator that helps to set duration almost in a conversational manner. In future iterations, we can run automated pre-tests to determine audience sizes and baseline conversion rates, we can improve the statistics, we can make it a lot easier and relax experimenters from filling up what we have to do today. There is one more immediate benefit from setting the duration upfront, we are able to stop experiments early much before the set duration if results are good (or bad) enough. And we can do early stopping safely with again keeping errors at bay below 5%. I personally believe that this possibility of early stopping without introduction of further error into our decision making is worth it. Experimentation Protocol in Depth \u00b6 General aim of experimenting is to test a new idea (feature) on a fraction of users. Based on observed behavior of users exposed to the experiment, we want to infer what will happen if we full-scale the change to the whole user base. We are looking for the best option as well as we are limiting a risk of full-scaling harmful ideas (features). The experimenting is a work with uncertainty and thus it includes inevitable errors. We distinguish two types of errors in experimenting: Type I error (False-positive error) We DO detect falsely significant result as significant Null hypothesis is true and we reject it Type II error (False-negative error) We DO NOT detect truly significant result as significant Null hypothesis is false and we do not reject it The Experimentation Platform keeps false-positive errors below 5% by Implementation of sound statistical evaluation (engineers and data people). Asking experimenters to (dully) follow Experimentation Protocol . By ignoring Experimentation Protocol you can drastically increase false-positive errors (even multiple times) making results of the experiment highly misleading! There should not be done any business decision based on such results. When implementing the experimentation protocol, we followed a meta-study Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale that collects best-practices from the leading experimentation platforms in Booking.com, Microsoft, Intuit and others. We also studied how online platforms like Optimizely, Google Optimize, VWO, AB Smartly implement experimentation protocols to come up with the meaningful yet powerful minimum. The experimentation protocol requires the experimenter to adhere to following steps: Formulate Hypothesis Pick Primary Metric Set Experiment Duration Check Guardrail Metrics Formulate Hypothesis \u00b6 Experiment hypothesis should be defined and should be falsifiable. Experiment analysis begins with the transformation of an idea for testing into an experiment. The first step in this process is the impact analysis. Every change for testing should be introduced with a description of what the change that will be evaluated is, who will see the change, what the expected impact is, and how this impact is connected to the top-level business goals (increase in revenue). Most importantly, an impact analysis should contain a line of reasoning \u2013 belief - that explains why a change is expected to have an impact. Common questions that help in evaluating this item are \u201cWhy are we improving this?\u201d, \u201cWhy is existing state not good enough?\u201d, \u201cWhy will the change make a difference?\u201d, \u201cWill it help the users, the business, or both?\u201d 1 Pick Primary Metric \u00b6 Picking primary metric helps to frame the experiment by determining what is the most important to measure. It also helps to think about possible next steps after the experiment will be over. Think about what you will do in all 3 possible experiment results: Significant positive impact Significant negative impact Inconclusive results Set Experiment Duration Before Starting It \u00b6 The length of the experiment must be set upfront (before starting the experiment). If we do not set it up, then when should we stop? Immediately when the difference in the primary metric is significant? Or should we wait two more days and stop it then? Imagine you are developing a new version of a drug (medicament) and now you want to test a new version versus an old version. How will you proceed? Probably you will select a group of people and test it on them. If the difference between old and new versions is not statistically significant in a selected group of people, you end up experimenting with the result that the new drug is not outperforming the old one. But definitely you will not be adding more and more people to the experiment just to prove the new one is better. The same holds for online experiments. The experimenter must estimate the necessary duration of the experiment carefully. The best way to do it is to use the Sample Size Calculator implemented right in the Experimentation Platform, or ask colleagues, or ping EP Support team. The Experimentation Platform supports safe early-stopping of the experiment. If the experiment is performing really great or really poor, we can stop it right now and make a decision. We do not need to wait until the end of the experiment. For the right and safe set up of early-stopping criteria, we need experimenters to set up the appropriate duration of the experiment upfront. This is alpha and omega of controlling false-positive errors - falsely claiming untrue effect to be a true effect. See Sequential Analysis for detailed explanation. Check Guardrail Metrics \u00b6 Many things can go wrong during the experiment running phase. We might not get IID exposures in variants, data pipelines could fail, there could be client-side bugs in the experiment implementation. It is necessary to check for possible bugs before evaluating the experiment. While EP is not a platform to check all possible errors, it does the best to highlight and warn about them. EP does Sample Ratio Mismatch check for all experiments by default and it offers several guardrail metrics that could be part of the secondary metric definition. Sample Ratio Mismatch \u00b6 Checking for SRM-Sample Ratio Mismatch, is an important test everyone doing experimenting should do. If you designed your experiment with equal percentages and got 821,588 vs. 815,482 users (50.2% instead of 50%), stop and find the bug 2 . Failing SRM check tells us there is some underlying problem in the experiment randomization. Experiments with failed SRM check should not be evaluated at all. See SRM for details about EP implementation. Pros and Cons \u00b6 On one hand we require experimenters to set experiment duration upfront, on the other hand we allow for safely calling the experiment off any time when measured differences are big enough. Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale \u21a9 R. Kohavi, Sample Ratio Mismatch \u21a9","title":"Experimentation Protocol"},{"location":"user_guide/protocol.html#experimentation-protocol","text":"We try to be well aware that implementation of experimentation protocol is what all of us experimenters will find the most difficult to understand and get done correctly when running experiments in teams and scaling the experimentation up to the whole companies. The experimentation protocol we are implementing is here to rather enable us to safely run and evaluate more experiments than to limit us from expanding our experimentation. Experimentation is a work with uncertainty where errors and erroneous decisions were and will be a necessary part of it. When experimenting on a larger scale, errors happen. It\u2019s no longer a \u201csmall chance of error\u201d in case when I am running one or two experiments. Errors happen with a specific chance. This chance must be monitored and controlled for. It will always be true that we will be making wrong decisions based on experiment data. There is no way out of it. We can only control it. It is not about being statistically correct, it is that we can and should know that we are making the right decision 95 times and the wrong one 5 times out of every 100 decisions we make. Not knowing and controlling for this, we are playing a game where we do not know our odds of a good or a bad decision. Only by joint effort among systems, data, and experimenters, we can guarantee to keep erroneous decisions at bay. Decisions we make are the most important outcome from experimenting, we need to keep bad decisions at bay. To do that, we need to set experiment duration upfront and follow the experimentation protocol. Following experimentation protocol is not special to us, it is a proven concept from clinical trials and also from online controlled experiments, it ensures that we are making the right decisions most of the time. Setting experiment duration upfront is certainly the most difficult point. We are not sure if current implementation (still in development) is the best one, we will need to iterate based on your feedback. We will have a calculator that helps to set duration almost in a conversational manner. In future iterations, we can run automated pre-tests to determine audience sizes and baseline conversion rates, we can improve the statistics, we can make it a lot easier and relax experimenters from filling up what we have to do today. There is one more immediate benefit from setting the duration upfront, we are able to stop experiments early much before the set duration if results are good (or bad) enough. And we can do early stopping safely with again keeping errors at bay below 5%. I personally believe that this possibility of early stopping without introduction of further error into our decision making is worth it.","title":"Experimentation Protocol"},{"location":"user_guide/protocol.html#experimentation-protocol-in-depth","text":"General aim of experimenting is to test a new idea (feature) on a fraction of users. Based on observed behavior of users exposed to the experiment, we want to infer what will happen if we full-scale the change to the whole user base. We are looking for the best option as well as we are limiting a risk of full-scaling harmful ideas (features). The experimenting is a work with uncertainty and thus it includes inevitable errors. We distinguish two types of errors in experimenting: Type I error (False-positive error) We DO detect falsely significant result as significant Null hypothesis is true and we reject it Type II error (False-negative error) We DO NOT detect truly significant result as significant Null hypothesis is false and we do not reject it The Experimentation Platform keeps false-positive errors below 5% by Implementation of sound statistical evaluation (engineers and data people). Asking experimenters to (dully) follow Experimentation Protocol . By ignoring Experimentation Protocol you can drastically increase false-positive errors (even multiple times) making results of the experiment highly misleading! There should not be done any business decision based on such results. When implementing the experimentation protocol, we followed a meta-study Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale that collects best-practices from the leading experimentation platforms in Booking.com, Microsoft, Intuit and others. We also studied how online platforms like Optimizely, Google Optimize, VWO, AB Smartly implement experimentation protocols to come up with the meaningful yet powerful minimum. The experimentation protocol requires the experimenter to adhere to following steps: Formulate Hypothesis Pick Primary Metric Set Experiment Duration Check Guardrail Metrics","title":"Experimentation Protocol in Depth"},{"location":"user_guide/protocol.html#formulate-hypothesis","text":"Experiment hypothesis should be defined and should be falsifiable. Experiment analysis begins with the transformation of an idea for testing into an experiment. The first step in this process is the impact analysis. Every change for testing should be introduced with a description of what the change that will be evaluated is, who will see the change, what the expected impact is, and how this impact is connected to the top-level business goals (increase in revenue). Most importantly, an impact analysis should contain a line of reasoning \u2013 belief - that explains why a change is expected to have an impact. Common questions that help in evaluating this item are \u201cWhy are we improving this?\u201d, \u201cWhy is existing state not good enough?\u201d, \u201cWhy will the change make a difference?\u201d, \u201cWill it help the users, the business, or both?\u201d 1","title":"Formulate Hypothesis"},{"location":"user_guide/protocol.html#pick-primary-metric","text":"Picking primary metric helps to frame the experiment by determining what is the most important to measure. It also helps to think about possible next steps after the experiment will be over. Think about what you will do in all 3 possible experiment results: Significant positive impact Significant negative impact Inconclusive results","title":"Pick Primary Metric"},{"location":"user_guide/protocol.html#set-experiment-duration-before-starting-it","text":"The length of the experiment must be set upfront (before starting the experiment). If we do not set it up, then when should we stop? Immediately when the difference in the primary metric is significant? Or should we wait two more days and stop it then? Imagine you are developing a new version of a drug (medicament) and now you want to test a new version versus an old version. How will you proceed? Probably you will select a group of people and test it on them. If the difference between old and new versions is not statistically significant in a selected group of people, you end up experimenting with the result that the new drug is not outperforming the old one. But definitely you will not be adding more and more people to the experiment just to prove the new one is better. The same holds for online experiments. The experimenter must estimate the necessary duration of the experiment carefully. The best way to do it is to use the Sample Size Calculator implemented right in the Experimentation Platform, or ask colleagues, or ping EP Support team. The Experimentation Platform supports safe early-stopping of the experiment. If the experiment is performing really great or really poor, we can stop it right now and make a decision. We do not need to wait until the end of the experiment. For the right and safe set up of early-stopping criteria, we need experimenters to set up the appropriate duration of the experiment upfront. This is alpha and omega of controlling false-positive errors - falsely claiming untrue effect to be a true effect. See Sequential Analysis for detailed explanation.","title":"Set Experiment Duration Before Starting It"},{"location":"user_guide/protocol.html#check-guardrail-metrics","text":"Many things can go wrong during the experiment running phase. We might not get IID exposures in variants, data pipelines could fail, there could be client-side bugs in the experiment implementation. It is necessary to check for possible bugs before evaluating the experiment. While EP is not a platform to check all possible errors, it does the best to highlight and warn about them. EP does Sample Ratio Mismatch check for all experiments by default and it offers several guardrail metrics that could be part of the secondary metric definition.","title":"Check Guardrail Metrics"},{"location":"user_guide/protocol.html#sample-ratio-mismatch","text":"Checking for SRM-Sample Ratio Mismatch, is an important test everyone doing experimenting should do. If you designed your experiment with equal percentages and got 821,588 vs. 815,482 users (50.2% instead of 50%), stop and find the bug 2 . Failing SRM check tells us there is some underlying problem in the experiment randomization. Experiments with failed SRM check should not be evaluated at all. See SRM for details about EP implementation.","title":"Sample Ratio Mismatch"},{"location":"user_guide/protocol.html#pros-and-cons","text":"On one hand we require experimenters to set experiment duration upfront, on the other hand we allow for safely calling the experiment off any time when measured differences are big enough. Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale \u21a9 R. Kohavi, Sample Ratio Mismatch \u21a9","title":"Pros and Cons"},{"location":"user_guide/quick_start.html","text":"Installation \u00b6 You can install this package via pip . pip install ep-stats Running \u00b6 You can run a testing version of ep-stats via python -m epstats Then see Swagger on http://localhost:8080/docs for API documentation. Contributing \u00b6 To get started locally, you can clone the repo and quickly get started using the Makefile . git clone https://github.com/avast/ep-stats.git cd ep-stats make install-dev It sets a new virtual environment venv in ./venv using venv , installs all development dependencies, and sets pre-commit git hooks to keep the code neatly formatted with flake8 and brunette . To run tests, you can use Makefile as well. source venv/bin/activate # activate python environment make check To run a development version of ep-stats do source venv/bin/activate cd src python -m epstats Base Example \u00b6 Ep-stats allows for quick experiment evaluation. We are using provided testing data to evaluate metric Click-through Rate in experiment test-conversion . from epstats.toolkit import Experiment , Metric , SrmCheck experiment = Experiment ( 'test-conversion' , 'a' , [ Metric ( 1 , 'Click-through Rate' , 'count(test_unit_type.unit.click)' , 'count(test_unit_type.global.exposure)' ), ], [ SrmCheck ( 1 , 'SRM' , 'count(test_unit_type.global.exposure)' )], unit_type = 'test_unit_type' ) # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData . load_goals_agg ( experiment . id ) # evaluate experiment ev = experiment . evaluate_agg ( goals ) ev contains evaluations of exposures, metrics and checks. This will have following output. ev.exposures : exp_id exp_variant_id exposures test-conversion a 21 test-conversion b 26 ev.metrics : exp_id metric_id metric_name exp_variant_id count mean std sum_value confidence_level diff test_stat p_value confidence_interval standard_error degrees_of_freedom test-conversion 1 Click-through Rate a 21 0.238095 0.436436 5 0.95 0 0 1 1.14329 0.565685 40 test-conversion 1 Click-through Rate b 26 0.269231 0.452344 7 0.95 0.130769 0.223152 0.82446 1.18137 0.586008 43.5401 ev.checks : exp_id check_id check_name variable_id value test-conversion 1 SRM p_value 0.465803 test-conversion 1 SRM test_stat 0.531915 test-conversion 1 SRM confidence_level 0.999000","title":"Quick Start"},{"location":"user_guide/quick_start.html#installation","text":"You can install this package via pip . pip install ep-stats","title":"Installation"},{"location":"user_guide/quick_start.html#running","text":"You can run a testing version of ep-stats via python -m epstats Then see Swagger on http://localhost:8080/docs for API documentation.","title":"Running"},{"location":"user_guide/quick_start.html#contributing","text":"To get started locally, you can clone the repo and quickly get started using the Makefile . git clone https://github.com/avast/ep-stats.git cd ep-stats make install-dev It sets a new virtual environment venv in ./venv using venv , installs all development dependencies, and sets pre-commit git hooks to keep the code neatly formatted with flake8 and brunette . To run tests, you can use Makefile as well. source venv/bin/activate # activate python environment make check To run a development version of ep-stats do source venv/bin/activate cd src python -m epstats","title":"Contributing"},{"location":"user_guide/quick_start.html#base-example","text":"Ep-stats allows for quick experiment evaluation. We are using provided testing data to evaluate metric Click-through Rate in experiment test-conversion . from epstats.toolkit import Experiment , Metric , SrmCheck experiment = Experiment ( 'test-conversion' , 'a' , [ Metric ( 1 , 'Click-through Rate' , 'count(test_unit_type.unit.click)' , 'count(test_unit_type.global.exposure)' ), ], [ SrmCheck ( 1 , 'SRM' , 'count(test_unit_type.global.exposure)' )], unit_type = 'test_unit_type' ) # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData . load_goals_agg ( experiment . id ) # evaluate experiment ev = experiment . evaluate_agg ( goals ) ev contains evaluations of exposures, metrics and checks. This will have following output. ev.exposures : exp_id exp_variant_id exposures test-conversion a 21 test-conversion b 26 ev.metrics : exp_id metric_id metric_name exp_variant_id count mean std sum_value confidence_level diff test_stat p_value confidence_interval standard_error degrees_of_freedom test-conversion 1 Click-through Rate a 21 0.238095 0.436436 5 0.95 0 0 1 1.14329 0.565685 40 test-conversion 1 Click-through Rate b 26 0.269231 0.452344 7 0.95 0.130769 0.223152 0.82446 1.18137 0.586008 43.5401 ev.checks : exp_id check_id check_name variable_id value test-conversion 1 SRM p_value 0.465803 test-conversion 1 SRM test_stat 0.531915 test-conversion 1 SRM confidence_level 0.999000","title":"Base Example"},{"location":"user_guide/test_data.html","text":"Test Data \u00b6 We made testing data part of the epstats python package to simplify development and to provide real example of input data and formats required by epstats . There are test goal data in both pre-aggregated and by-unit forms. See TestData for various access methods. Test data itself are saved as csv files in src/epstats/toolkit/testing/resources . They include pre-aggregated and by-unit goals together with pre-computed evaluations of metrics, checks, and exposures that are used to assert our unit-tests against (e.g. in test_experiment.py ). How to Update Test Data \u00b6 We keep master of test data in google spreadsheet because we implemented statistical procedure in the sheet itself to pre-calculate experiment evaluations we then use in unit test asserts.","title":"Test Data"},{"location":"user_guide/test_data.html#test-data","text":"We made testing data part of the epstats python package to simplify development and to provide real example of input data and formats required by epstats . There are test goal data in both pre-aggregated and by-unit forms. See TestData for various access methods. Test data itself are saved as csv files in src/epstats/toolkit/testing/resources . They include pre-aggregated and by-unit goals together with pre-computed evaluations of metrics, checks, and exposures that are used to assert our unit-tests against (e.g. in test_experiment.py ).","title":"Test Data"},{"location":"user_guide/test_data.html#how-to-update-test-data","text":"We keep master of test data in google spreadsheet because we implemented statistical procedure in the sheet itself to pre-calculate experiment evaluations we then use in unit test asserts.","title":"How to Update Test Data"}]}